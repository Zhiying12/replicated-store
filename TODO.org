#+title: TODO

* TODO
** Rust
   - consider using scoped threads https://doc.rust-lang.org/stable/std/thread/fn.scope.html
** Java
*** avoid class duplication
    - currently we have both proto generated and custom implementations for
      Instance and Command; We transform from custom implementation to proto
      generated one. Refactor such that we have only proto generated classes.

** C++
*** avoid holding the lock for a long time in rpc handlers

*** explore the overhead of locking ballot_; replace NextBallot with CAS loop,
    - remove the lock, and change ballot to atomic and measure the overhead.

*** consider replacing ballot with atomic and using the CAS loop
    - and make sure to insert PAUSEs below, per recommendation of Thiago on C++
      slack: https://herbsutter.com/2012/08/31/reader-qa-how-to-write-a-cas-loop-using-stdatomics/


*** consider merging xxx_state_t variables into one
    - using std::variant and using the same function for quorum detection in the
      while loop at the end of SendXXX functions.

*** currently, replicant hangs on accept() and we stop the program using C-c.
    - this has further implications because, e.g., a standalone heartbeat thread
      will not be able to know when to stop; this is not a problem because the
      OS will clean up if we C-c and exit the main thread, but a cleaner
      solution is  desirable.

*** reimplement concurrency stuff using C++20 features.

*** handle errors in asio calls

** All
*** reorder the code in tests/doc/code to match the protocol
    - currently things are mixed up with commit stuff followed by prepare and
      then accept, mainly due to our rename of heartbeat to commit; fix the
      order in all files
*** remove sender_id field from protobuf requests
*** avoid sleeps in tests due to TCP connection re-establishment

*** handle gaps due to temporary disconnect
    - Currently, if a peer temporarily disconnects and then reconnects, then it
      will have a gap in its log. it will not be able to execute entries past
      the gap, it will not be able to prune its log, which will prevent everyone
      else from pruning their logs. when we have a gap like this, we should
      either (1) force a new leader election to recover the lost entries, or (2)
      ask other peers and fill our log, or (3) resort to using log pruning that
      persists the state machine to disk and prunes the log without hearing from
      the peers. we do not implement this at the moment: if a peer temporarily
      disconnects and accrues a gap, then log pruning will be stuck on all
      processes.

*** handle timeouts in RPCs
    - with the most recent design (heap-allocated refcounted state), this should
      just work, but we need to actually try and see if it works.

*** rethink class interface and hide methods that don't need to be public


*** explore the cost of sending RPCs to self

*** evaluate the choice of a resizable circular buffer for log
    - see how boost implements it

*** async version

* DONE

*** DONE avoid tsan warnings due to stale threads
    - The current design may push stale prepare responses to prepare_ok_reponses
      and increase the prepare_num_responses. Then the prepare_thread proceed to
      replay as it reaches prepare_num_responses, though the instances for
      replay are not from majority peers. (heartbeat_thread and accept might
      have similar issues, though it seems that they won't fail the correctness
      so far)
    - we fixed it by storing the common state among threads in a heap-allocated
      and reference counted struct that is shared among threads; the last thread
      that exits frees the state

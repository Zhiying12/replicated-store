- MultiPaxos

  - types/protobufs

    - commit_request
      - ballot_: ballot of the leader sending the commit RPC
      - last_executed_: last_executed_ of the leader sending the commit RPC
      - global_last_executed_: global_last_executed of the leader sending the
        commit RPC
      - sender: id of the sender

    - commit_response
      - type_: enum {ok, reject}
      - ballot_: ballot of the peer (valid only if type_ == reject)
      - last_executed_: last_executed_ of the follower responding to the commit
        RPC

    - prepare_request
      - ballot_: ballot of the sender
      - sender: id of the sender

    - prepare_response
      - type_: enum {ok, reject}
      - ballot_: ballot of the peer (valid only if type_ == reject)
      - instances_: Instances of the peer since global_last_executed_ (valid
        only if type_ == ok)

    - accept_request
      - instance_: Instance sent by the leader
      - sender_: id of the leader

    - accept_response
      - type_: enum {ok, reject}
      - ballot_: ballot of the peer (valid only if type_ == reject)

  - members
    - ready_: atomic<bool>; a flag that indicates if the peer---after becoming a
      leader---is ready to respond to client requests. we need this flag because
      once a peer becomes a leader, it may not yet be able to serve client
      requests since it may need to recover the log and replay the instances it
      received as a response to the prepare request, to catch up with the rest
      of the peers. during this replay, the ready_ flag is false and this peer
      responds to the clients with "retry". as soon as this peer has caught up,
      we set ready_ to true and start responding to the clients.

    - ballot_: int64_t; current ballot number known to the peer;
      initialized |max_num_peers_|, which indicates that there is no current
      leader, since valid leader ids are in [0, max_num_peers_). it is a 64-bit
      integer, the lower 8 bits of which is always equal to |id_| of the peer
      that chose |ballot_|, and the higher bits represent the round number. we
      preserve 8 bits for |id_| but limit |id_| to 4 bits to avoid an overflow
      when identifying a leader. initialized to |id_|. at any moment, we can
      look at the lower 8 bits of |ballot_| to determine current leader.

    - log_: non-owning pointer to Log instance.

    - id_: int64; identifier of this peer. initialized from the configuration
      file. currently, we limit the number of peers to 16; therefore, id_ is a
      value in [0, 16).

    - commit_received_: atomic<bool>; indicates whether a commit message was
      received during commit_interval_.

    - commit_interval_: milliseconds; time between sending consecutive commits.
      initialized from the configuration file.

    - port_: string; the port where the RPC server is listening

    - rpc_peers_: an array of RPC endpoints to peers, including to self.
      initialized from the configuration file.

    - mu_: mutex; MultiPaxos is a concurrent object -- multiple threads may
      concurrently call its |replicate| method. |mu_| protects shared data in
      MultiPaxos.

    - tp_: thread_pool; a thread pool to which we post RPC requests to peers;
      initialized from the configuration file.

    - cv_leader_: condition variable; commit_thread sleeps on this condition
      variable to get notified when this peer becomes a leader.

    - cv_follower_: condition variable; prepare_thread sleeps on this condition
      variable to get notified when this peer becomes a follower.

    - rpc_server_: a RPC server for handling incoming RPC requests; initialized
      based on the configuration file

    - rpc_server_running_: bool; a flag that indicates if the RPC server is
      running

    - rpc_server_running_cv_: condition variable; to avoid calling gRPC's
      shutdown() on the RPC server before it is started, we sleep on this
      condition variable in the stop_rpc_server() method; in the
      start_rpc_server() method, we signal this condition variable after
      starting the RPC server.

    - rpc_server_thread_: thread; runs the rpc server until shutdown() is
      called.

    - prepare_thread_running_: atomic<bool>; a flag that indicates if the
      prepare thread is running

    - prepare_thread_: thread; runs the function prepare_thread() until
      shutdown() is called.

    - commit_thread_running_: atomic<bool>; a flag that indicates if the commit
      thread is running

    - commit_thread_: thread; runs the function commit_thread() until shutdown()
      is called.

  - constants

    - id_bits_ = 0xff: the lower bits of |ballot_| we use for storing the id of
      the current leader.

    - round_increment_ = id_bits_ + 1: we add this constant to |ballot_| to
      increment the round portion of |ballot_| by one.

    - max_num_peers_ = 0xf: maximum number of peers.

  - methods

    - constructor(log: *Log, cfg: config)
      ready_ = false
      ballot_ = max_num_peers_
      log_ = log
      id_ = config["id"]
      commit_received_ = false
      commit_interval = config["commit_interval"]
      port_ = config["peers"][id_]
      rpc_server_running_ = false
      prepare_thread_running_ = false
      commit_thread_running_ = false
      instantiate RPC stubs to each peer including self

    - start()
      start_prepare_thread()
      start_commit_thread()
      start_rpc_server()

    - stop()
      stop_rpc_server()
      stop_prepare_thread()
      stop_commit_thread()
      tp_.join()

    - start_rpc_server()
      # builds the RPC server and assigns it to rpc_server_, sets
      # rpc_server_running_ to true and signals rpc_server_running_cv_ in case
      # stop_rpc_server() was called first; starts the RPC server by calling
      # gRPC wait method in a separate thread so that this thread can continue
      rpc_server_ = build rpc server
      mu_.lock()
      rpc_server_running_ = true
      rpc_server_running_cv_.notify_one()
      mu_.unlock()
      rpc_server_thread_ = new thread ( rpc_server->wait() )

    - stop_rpc_server()
      # waits until the RPC server is started and then calls shutdown() on the
      # rpc server handle.
      mu_.lock()
      while !rpc_server_running_
        rpc_server_running_cv_.wait(mu_)
      rpc_server_.shutdown()
      rpc_server_thread_.join()

    - start_prepare_thread()
      # starts a thread, called prepare_thread_ to run the prepare_thread()
      # function
      prepare_thread_running_ = true
      prepare_thread_ = new thread ( prepare_thread() )

    - stop_prepare_thread()
      # stops prepare_thread_. first sets prepare_thread_running_ to false and
      # then notifies cv_follower_ because prepare_thread() function may be
      # asleep on it.
      mu_.lock()
      prepare_thread_running_ = false
      cv_follower_.notify_one()
      mu_.unlock()
      prepare_thread_.join()

    - start_commit_thread()
      # starts a thread, called commit_thread_ to run the commit_thread()
      # function
      commit_thread_running_ = true
      commit_thread_ = new thread ( commit_thread() )

    - stop_commit_thread()
      # stops commit_thread_. first sets commit_thread_running_ to false and
      # then notifies cv_leader_ because commit_thread() function may be asleep
      # on it.
      mu_.lock()
      commit_thread_running_ = false
      cv_leader_.notify_one()
      mu_.unlock()
      commit_thread_.join()

    - next_ballot() -> int
      # description: gets the next ballot number by incrementing the round
      # portion of |ballot_| by |round_increment_| and setting the |id_| bits to
      # the id if this peer, since |ballot_| could have been generated by
      # another peer.
      mu_.lock()
      next_ballot = ballot_
      next_ballot += round_increment_
      next_ballot = (next_ballot & ~id_bits_) | id_
      mu_.unlock()
      return next_ballot

    - become_leader(ballot: int)
      # this function is called at the end of a successful prepare phase, once
      # we have received prepare responses from the majority. it sets the
      # ballot_ to its argument, which is the new ballot number, and it sets
      # ready_ to false because this peer needs to replay all the instances it
      # has received from peers in response to prepare message and until all
      # those instances are replayed, this new leader is not ready to answer the
      # client's queries. it also notifies cv_leader_ so that the commit thread
      # would start sending out commits to the peers to suppress new leader
      # elections.
      mu_.lock()
      ballot_ = ballot
      ready_ = false
      cv_leader_.notify_one()


    - is_leader() -> bool
      # description: returns true if this peer is leader
      # preconditions:
      # postconditions:
      mu_.lock() # unlocks on return
      return is_leader_lockless()

    - is_leader_lockless() -> bool
      # description: returns true if this peer is leader
      # preconditions: lock is not acquired
      # postconditions:
      return (ballot_ & id_bits) == id_

    - is_someone_else_leader() -> bool
      # description: returns true if we are not a leader and someone else is.
      # this cannot simply be !is_leader() because it's not necessarily the case
      # that if this peer is not leader, then someone else must be: at startup
      # there is no leader.
      # preconditions:
      # postconditions:
      mu_.lock() # unlocks on return
      id = ballot_ & id_bits_
      return id != id_ && id < max_num_peers_

    - wait_until_leader()
      # description: waits until this peer becomes a leader
      # preconditions:
      # postconditions:
      mu_.lock()
      while running_ && !is_leader_lockless()
        cv_leader_.wait(mu_)
      mu_.unlock()

    - replicate(cmd: command, client-id: int) -> accept_result_t
      if i_am_leader()
        if ready_
          return accept(cmd, log_.advance_last_index(), client-id)
        return accept_result_t{type_: retry, leader_: N/A}
      if someone_else_is_leader()
        return accept_result_t{type_: reject, leader_: leader()}
      # election in progress
      return accept_result_t{type_: retry, leader_: N/A}

    - commit_handler(request: commit_rpc_request)
      # description: handler for the commit RPC
      # preconditions:
      # postconditions:

      mu_.lock() # unlocks on return
      if request.ballot_ >= ballot_
        last_commit_ = time::now()
        ballot_ = request.ballot_
        stale_rpc = false
        log_.commit_until(request_.last_executed_, ballot_)
        log_.trim_until(request_.global_last_executed)
      return commit_rpc_response{last_executed_: log_.last_executed()}

    - commit_thread()
      while (running_)
        wait_until_leader()
        # this peer is a leader now
        #
        # initialize global_last_executed and update it later on each iteration;
        # we send a commit to ourselves as well, which results in trimming of
        # our log.
        global_last_executed = log_.global_last_executed()
        while (running_)
          commit_num_rpcs_ = 0
          commit_responses_ = []

          mu_.lock()
          commit_request_.set_ballot(ballot_)
          mu_.unlock
          commit_request_.set_last_executed(log_.last_executed())
          commit_request_.set_global_last_executed(global_last_executed)

          for each peer p
            post a closure to a thread pool {
              status = p.Commit(commit_request_)
              commit_mu_.lock()
              ++commit_num_rpcs_
              if status.ok
                commit_responses_.push_back(status.response)
              commit_cv_.notify_one()
            }
            commit_mu_.unlock()

          commit_mu_.lock()
          # for commits, we have to wait until we have sent rpcs to all of
          # the peers; this is unlike accept or prepare rpcs because with
          # commits we receive last_executed from peers, and if we want to
          # advance global_last_executed, we need to receive last_executed from
          # all peers so that we could be sure the new global_last_executed is a
          # global minimum.
          while (is_leader() && commit_num_rpcs_ != peers_.size())
            commit_cv_.wait(hb_mu)
          if commit_responses_.size() == peer_.size()
            global_last_executed = min(ok_responses)
          commit_mu_.unlock()

          sleep(commit_interval_)
          if !is_leader()
            break

    - prepare_handler(request: prepare_rpc_request)
      # description: handler for the prepare RPC
      # preconditions:
      # postconditions:

      mu_.lock() # unlocks on return
      if request.ballot_ >= ballot_
        ballot_ = request_.ballot_
        return prepare_rpc_response{type_: ok,
                                    ballot_: N/A,
                                    log_: log_.instances_since_global_last_executed()}
      # reject stale RPC requests
      return prepare_rpc_response{type_: reject, ballot_: ballot_, log_: N/A}


    - prepare() -> prepare_result_t:
      num_responses = 0
      ok_logs = vector<instance_t[]>
      cv, mu
      request = prepare_rpc_request{ballot_: next_ballot#()}
      for each peer p {
        run closure in a separate thread {
          response = p.prepareRPC(request)
          lock(mu)
          ++num_responses
          if response.type_ == ok:
            ok_logs.push(response.log_)
          else if response.type_ == reject:
            ballot_ = response.ballot_
          # else it is a timeout error; we do nothing
          unlock(mu)
          cv.notify_one()
        }
      }
      lock(mu)
      while i_am_leader() &&
            num_ok_responses <= peers_.size()/2 &&
            num_responses != peers_.size()
        cv.wait(mu)
      # one of the above three conditions is false; handle each, starting with the
      # most likely one
      if num_ok_responses > peers_.size()/2: # we have quorum
        return prepare_result_t{type_: ok, log_: ok_logs}
      if someone_else_is_leader():
        return prepare_result_t{type_: reject}
      # multiple timeout responses
      return prepare_result_t{type_: timeout}

    - prepare_thread()
      for (;;) {
        sleep until follower
        for (;;) {
          sleep(commit_interval_ + random(10, commit_interval_))
          if time::now() - last_commit_ < commit_interval_:
            continue
          prepare_result_t result = prepare()
          if result.type_ != ok:
            continue
          # we are a leader
          wake up commit_thread
          ready_ = false
          log_.merge(result.logs_)
          if (replay())
            ready_ = true
          break
        }
      }

    - accept_handler(request: accept_rpc_request)
      # description: handler for the accept RPC
      # preconditions:
      # postconditions:

      if request.ballot_ >= ballot_:
        ballot_ = request.ballot_
        log_.append(request.instance_)
        return accept_rpc_response{type_: ok, ballot_: N/A}
      # stale message
      return accept_rpc_response{type_: reject, ballot_: ballot_}

    - accept(cmd: command, index: int, client-id: int) -> accept_result_t
      num_responses = 0
      num_ok_responses = 0
      cv, mu
      request = accept_rpc_request{command_: cmd,
                                   index_: index,
                                   ballot_: ballot_,
                                   client-id_: client-id}
      for each peer p {
        run closure in a separate thread {
          response = p.acceptRPC(request)
          lock(mu)
          ++num_responses
          if response.type_ == ok:
            ++ok_responses
          else if response.type_ == reject:
            ballot_ = response.ballot_
          # else it is a timeout error; we do nothing
          unlock(mu)
          cv.notify_one()
        }
      }
      lock(mu)
      while i_am_leader() &&
            num_ok_responses <= peers_.size()/2 &&
            num_responses != peers_.size():
        cv.wait(mu)

      if num_ok_responses > peers_.size() / 2
        log_.commit(index)
        return accept_result_t{type_: ok, leader_: N/A}
      if someone_else_is_leader()
        return accept_result_t{type_: reject, leader_: leader()}
      # RPCs timed out
      return accept_result_t{type_: retry, leader_: N/A}

    - replay() -> bool
      for i in log_.new_instances()
        accept_result_t r = accept(i.command_, i.index, i.client-id_)
        if r.type_ == leader
          return false
        if r.type_ == retry
          continue
      return true


- unit tests

  - constructor
    - check that class members are initialized correctly.

  - next_ballot
    - check that unique and higher ballot numbers are generated by different
      paxos peers.

  - requests_with_lower_ballot_ignored
    - check that stale rpcs (those with older than peer's ballot numbers) are
      ignored by the commit rpc handler:

      - in thread t0 start peer0 rpc server
      - in thread main, call peer0.next_ballot twice
      - in thread main, create an rpc request and set its ballot to the
        result of calling peer1.next_ballot
      - in thread main, invoke prepare, accept, commit rpcs on peer0

      since peer1.next_ballot was called once, the ballot on the RPCs should be
      smaller than the ballot on peer0; therefore, the rpcs should be ignored
      and peer0 should remain the leader.

  - requests_with_higher_ballot_change_leader_to_follower
    - start peer0 as a leader, and check that an rpc with higher than peer0's
      ballot number from peer1 changes the peer0 to follower and that peer0
      considers peer1 to be the leader.

      - in thread t0, start peer0 rpc server
      - in thread main, call peer0_.next_ballot to make peer0 leader
      - in thread main, create an rpc request and set its ballot to the result
        of calling peer1.next_ballot -- now peer1 is a leader with a higher
        ballot number than peer0, but peer0 does not yet know it.
      - in thread main, send the rpc request to peer0
      - check that peer0 is not a leader anymore and considers peer1 as the
        leader.
      - repeat the above for prepare, accept, and commit

  - commit_commits_and_trims
    - append to peer0's log three in-progress instances with indexes 1, 2, and 3
    - send a commit rpc to peer0 with last_executed = 2 and
      global_last_executed = 0
    - check that the last_executed in response is 0 and instances 1 and 2 are in
      committed state and instance 3 is in in-progress state
    - execute instances 1 and 2
    - send another commit rpc to peer0 with last_executed = 2 and
      global_last_executed = 2
    - check that the last_executed in response is 2 and instances 1 and 2 are
      trimmed

  - prepare_responsds_with_correct_instances
    - append to peer0's log three in-progress instances with indexes 1, 2, and 3
    - send a prepare rpc to peer0 and check that the response contains instances
      1, 2, and 3
    - send a commit rpc to peer0 with last_executed = 2 and
      global_last_executed = 0
    - instances 1 and 2 should be committed now; execute them
    - send a prepare rpc to peer0 and check that the response contains instances
      1 and 2 in executed state and instance 3 in in-progress state
    - send another commit rpc to peer0 with last_executed = 2 and
      global_last_executed = 2; peer0 should trim instances 1 and 2
    - send a prepare rpc to peer0 and check that the response contains only the
      instance 3

  - accept_appends_to_log
    - send an accept rpc to peer0 with an instance at index 1 and check that
      peer0's log contains the instance
    - send an accept rpc to peer0 with an instance at index 2 and check that
      peer0's log contains both instances

  - prepare_response_with_higher_ballot_changes_leader_to_follower
    - call peer0_.next_ballot to make it a leader
    - call peer1_.next_ballot to make it a leader
    - call peer2_.next_ballot to make it a leader
    - send a commit with peer2_'s ballot to peer1_ to establish peer2 as a
      leader in peer1 as well
    - call run_prepare_phase from peer0_ and expect it to become a follower
      assume peer2 as the leader

  - accept_response_with_higher_ballot_changes_leader_to_follower
    - call peer0_.next_ballot to make it a leader
    - call peer1_.next_ballot to make it a leader
    - call peer2_.next_ballot to make it a leader
    - send a commit with peer2_'s ballot to peer1_ to establish peer2 as a
      leader in peer1 as well
    - call run_accept_phase from peer0_ and expect it to become a follower
      assume peer2 as the leader

  - commit_response_with_higher_ballot_changes_leader_to_follower
    - call peer0_.next_ballot to make it a leader
    - call peer1_.next_ballot to make it a leader
    - call peer2_.next_ballot to make it a leader
    - send a commit with peer2_'s ballot to peer1_ to establish peer2 as a
      leader in peer1 as well
    - call run_commit_phase from peer0_ and expect it to become a follower
      assume peer2 as the leader

  - run_prepare_phase
    - check that run_prepare_phase returns null when it hasn't received
      responses from quorum, and it returns a properly merged log when it has
      heard from the quorum. to this end, we prepare a logs in peers that covers
      all valid scenarios and check that the final log produced from prepare
      phase is the correct one.
      - index1: peer0 and peer1 have the same instance, which should also appear
        in the returned log at index1
      - index2: peer0 has nothing, and peer 1 has an instance, which should
        appear in the returned log at index2
      - index3: peer0 has a committed instance and peer1 has the same command
        but with a higher ballot; the result depends on who responds first to
        prepare command: if peer0 responds first, then the committed instance
        will be inserted to the merged log first and peer1's instance with a
        higher ballot will be ignored; otherwise, peer1's instance will be
        inserted to the merged log first and peer0's instance will be ignored;
        eventually, we will have the same command at index3 but the ballot
        number may differ
      - index4: is the same as index3 except peer0 has an executed instance,
        instead of committed instance
      - index5: peer0 has instance with ballot n and peer1 has instance with
        ballot m, where n < m and all instances in in-progress state; the merged
        log will have peer1's instance in the log
      - start peer0 rpc server
      - call run_prepare_phase and expect null as the response
      - start peer1 rpc server
      - call run_prepare_phase and expect a response with a merged log as
        described above

  - run_accept_phase
    - check that run_accept_phase returns retry when it hasn't heard back from
      the quorum, and it returns ok otherwise
      - start peer0 rpc server
      - call peer0.run_accept_phase and expect retry as the response and peer0's
        loghas the instance at index 1 in in-progress state and peer1 and peer2
        have no entries at index 1
      - start peer1 rpc server
      - call peer0.run_accept_phase and expect ok as the response and peer0's
        log has the instance at index 1 in committed state and peer1 in
        in-progress state and peer2 has no entry at index 1.

  - run_commit_phase
    - check that run_commit_phase returns the passed in global_last_executed
      when it hasn't heard from all of the peers, and it returns the new
      global_last_executed when it has heard from all of the peers.
      - start peer0 and peer1 rpc servers
      - append to peer0's log committed instances at index 1, 2, and 3, and
        execute all instances
      - append to peer1's log committed instances at index 1, 2, and 3, and
        execute all instances
      - append to peer2's log committed instances at index 1 and 2, and execute
        all instances
      - call run_commit_phase with global_last_executed = 0 and expect 0 as the
        response
      - start peer2 rpc server
      - append to peer2 an in-progress instance at index 3
      - call run_commit_phase with global_last_executed = 0 and expect 2 as the
        response
      - peer2 should have now committed the instance at index3; execute it
      - call run_commit_phase with global_last_executed = 2 expect 3 as the
        response

  - replay
    - check that after replay peers have identical logs.
      - prepare a log with a committed entry with put command at index1,
        executed entry with get command at index2, and in in-progress entry with
        del command at index3
      - start peer0 and peer1 rpc servers
      - call peer0.replay with the log and new ballot and check that peer0
        contains committed entries at index1, index2, and index3 with put, get,
        and del commands, respectively; and peer1 contains the same commands
        in-inprogress state

  - replicate
    - check that replicate works correctly
      - start peer0's servers
      - call peer0.replicate and expect a retry response
      - start peer1 and peer2's servers
      - wait for 3 * commit_interval
      - confirm the leader has emerged and call it leader
      - call leader.replicate and expect ok
      - find a peer that is not a leader and call it nonleader
      - call nonleader.replicate and expect someone_else_leader response with
        leader

  - one_leader_elected
    - start peer0, peer1, peer2, wait for 3x of commit interval and then
      check that a single leader is elected.

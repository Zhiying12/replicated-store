- MultiPaxos
  # The Paxos module that contains the consensus logic for phase1 and phase2 of
  # the MultiPaxos protocol.

  - members

    - running_: atomic<bool>; a flag used by long-running threads (prepare
      thread and commit thread) to know when to stop running; initialized to
      false, set to true with the start() method and to false with the
      shutdown() method.

    - ready_: bool; a flag that indicates if the peer---after becoming a
      leader---is ready to respond to client requests. we need this flag because
      as soon as this peer calls next_ballot() in prepare_thread(), it becomes a
      leader, but it may not yet be able to serve client requests since it may
      need to recover the log and replay the instances it received as a response
      to prepare request, to catch up with the rest of the peers; during this
      replay, the ready_ flag is false and this peer responds to the clients
      with "retry"; as soon as this peer has caught up, we set ready_ to true
      and start responding to the clients. initialized to false; set to false in
      next_ballot() and set to true once replay has completed.

    - id_: int64_t; identifier of this peer. initialized from the configuration
      file. currently, we limit the number of peers to 16; therefore, id_ is a
      value in [0, 16).

    - port_: string; the port where the RPC server is listening

    - ballot_: int64_t; current ballot number known to the peer;
      initialized |max_num_peers_|, which indicates that there is no current
      leader, since valid leader ids are in [0, max_num_peers_). it is a 64-bit
      integer, the lower 8 bits of which is always equal to |id_| of the peer
      that chose |ballot_|, and the higher bits represent the round number. we
      preserve 8 bits for |id_| but limit |id_| to 4 bits to avoid an overflow
      when identifying a leader. initialized to |id_|. at any moment, we can
      look at the lower 8 bits of |ballot_| to determine current leader.

    - log_: Log; a non-owning pointer to Log instance.

    - peers_: array of RPC endpoints; RPC endpoints to peers, including to self.
      initialized from the configuration file.

    - ready_: boolean; when a new leader is elected, it receives logs from the
      peers in the quorum; the leader merges all these logs to obtain the most
      up-to-date instance at each index in the log and then replays (calls
      accept on) all the instances in the merged log. for now, we consider the
      leader not ready to accept new requests from the clients until it has
      processed the merged log; the |ready_| flag tracks the readiness for
      accepting commands from clients. initialized to false.

    - last_commit_: timestamp; timestamp of the last commit received from the
      current leader. initialized to 0.

    - commit_interval_: milliseconds; time between two consecutive commits.
      initialized from the configuration file.

    - rpc_peers_: stubs to RPC peers; initialized based on the configuration file

    - rpc_server_: an RPC server for handling incoming RPC requests; initialized
      based on the configuration file

    - mu_: mutex; MultiPaxos is a concurrent object -- multiple threads may
      concurrently call its |decide| method. |mu_| protects shared data in
      MultiPaxos.

    - tp_: thread_pool; a thread pool to which we post RPC requests to peers;
      initialized from the configuration file.

    # the followings are commit thread related variables. we cannot make them
    # local to the commit thread because the commit thread posts tasks to the
    # threadpool tp_; it is possible that the threads from the threadpool
    # outlive the commit thread, and if these variables are local to the commit
    # thread, then the threadpool threads will end up referencing destroyed
    # variables. One other possible solution is to have the threadpool local to
    # the commit thread as well.

    - commit_thread_: thread; runs the function commit_thread() until shutdown()
      is called.

    - cv_leader_: condition variable; commit_thread sleeps on this condition
      variable to get notified when this peer becomes a leader.

    - commit_request_: commit RPC request protobuf; initialized every time right
      before commits are sent by acquiring mu_ and getting a consistent set of
      its fields (ballot, last_executed, and global_last_executed) from this
      peer.

    - commit_num_rpcs_: int; the number of commit rpcs sent; initialized
      to 0 every time right before commits are sent.

    - commit_responses_: array of ints; the values of responses to commits;
      reset to empty every time right before commit are sent.

    - commit_mu_: mutex; serializes access to commit_num_responses_ and
      commit_ok_responses_ among thread pool threads and the commit thread.

    - commit_cv_: condition variable; the commit thread waits on this condition
      variable and it is woken up every time an RPC posted by a threadpool
      thread completes and signals this condition variable.

    # the followings are prepare thread related variables. we cannot make them
    # local to the prepare thread for the same reason as above, with the
    # commit thread.

    - prepare_thread_: thread; runs the function prepare_thread() until
      shutdown() is called.

    - cv_follower_: condition variable; prepare_thread sleeps on this condition
      variable to get notified when this peer becomes a follower.

    - prepare_num_rpcs_: int; the number of prepare rpcs sent; initialized to 0
      every time right before prepare rpcs are sent.

    - prepare_ok_responses_: array of log_vector_t; log_vector_ts received in a
      round of prepare rpcs

  - constants

    - id_bits_ = 0xff: the lower bits of |ballot_| we use for storing the id of
      the current leader.

    - round_increment_ = id_bits_ + 1: we add this constant to |ballot_| to
      increment the round portion of |ballot_| by one.

    - max_num_peers_ = 0xf: maximum number of peers.

  - types

    - commit_rpc_request
      - ballot_: ballot of the leader sending the commit RPC
      - last_executed_: last_executed_ of the leader sending the commit RPC
      - global_last_executed_: global_last_executed of the leader sending the commit RPC
      - sender: id of the sender

    - commit_rpc_response
      - last_executed_: last_executed_ of the follower responding to the
        commit RPC

    - prepare_rpc_request
      - ballot_: ballot of the sender
      - sender: id of the sender

    - prepare_rpc_response
      - type_: enum {ok, reject}
      - ballot_: ballot of the peer (valid only if type_ == reject)
      - instances_: Instances of the peer since global_last_executed_ (valid
        only if type_ == ok)

    - accept_rpc_request
      - instance: Instance sent by the leader
      - sender: id of the sender

    - accept_rpc_response
      - type_: enum {ok, reject}
      - ballot_: ballot of the peer (valid only if type_ == reject)

  - methods

    - constructor(log: *Log, cfg: config)
      running_ = false
      id_ = config["id"]
      port_ = config["peers"][id_]
      ballot_ = max_num_peers_
      log_ = log
      instantiate RPC stubs to each peer including self
      instantiate RPC server

    - start()
      # starts all the threads of the peer: RPC server threads, commit
      # threads, prepare threads. a multipaxos instance on which start() was
      # called, must have a corresponding shutdown() call for a graceful
      # shutdown; conversely, if start() wasn't called, then shutdown() must not
      # be called.
      running_ = true
      commit_thread_ = thread(commit_thread()) # start commit thread
      rpc_server_->wait() # start the RPC server so it listens for incoming calls

    - shutdown()
      running_ = false
      cv_leader_.notify_all()
      commit_thread_.join()
      rpc_server_->shutdown() # stop the RPC server

    - next_ballot() -> int
      # description: gets the next ballot number by incrementing the round
      # portion of |ballot_| by |round_increment_| and setting the |id_| bits to
      # the id if this peer, since |ballot_| could have been generated by
      # another peer. calling this method makes this peer the leader, which is
      # why we also signal the cv_leader_ condition variable to wake up the
      # long-running threads, commit_thread and prepare_thread.
      # preconditions:
      # postconditions:
      mu_.lock() # unlocks on return
      ballot_ += id_bits_
      ballot_ = (ballot_ & ~id_bits_) | id_
      cv_leader_.notify_all()
      return ballot_

    - leader() -> int
      # description: returns current leader's id
      # preconditions:
      # postconditions:
      mu_.lock() # unlocks on return
      return ballot_ & id_bits_

    - is_leader() -> bool
      # description: returns true if this peer is leader
      # preconditions:
      # postconditions:
      mu_.lock() # unlocks on return
      return is_leader_lockless()

    - is_leader_lockless() -> bool
      # description: returns true if this peer is leader
      # preconditions: lock is not acquired
      # postconditions:
      return (ballot_ & id_bits) == id_

    - is_someone_else_leader() -> bool
      # description: returns true if we are not a leader and someone else is.
      # this cannot simply be !is_leader() because it's not necessarily the case
      # that if this peer is not leader, then someone else must be: at startup
      # there is no leader.
      # preconditions:
      # postconditions:
      mu_.lock() # unlocks on return
      id = ballot_ & id_bits_
      return id != id_ && id < max_num_peers_

    - wait_until_leader()
      # description: waits until this peer becomes a leader
      # preconditions:
      # postconditions:
      mu_.lock()
      while running_ && !is_leader_lockless()
        cv_leader_.wait(mu_)
      mu_.unlock()

    - replicate(cmd: command, client-id: int) -> accept_result_t
      if i_am_leader()
        if ready_
          return accept(cmd, log_.advance_last_index(), client-id)
        return accept_result_t{type_: retry, leader_: N/A}
      if someone_else_is_leader()
        return accept_result_t{type_: reject, leader_: leader()}
      # election in progress
      return accept_result_t{type_: retry, leader_: N/A}

    - commit_handler(request: commit_rpc_request)
      # description: handler for the commit RPC
      # preconditions:
      # postconditions:

      mu_.lock() # unlocks on return
      if request.ballot_ >= ballot_
        last_commit_ = time::now()
        ballot_ = request.ballot_
        stale_rpc = false
        log_.commit_until(request_.last_executed_, ballot_)
        log_.trim_until(request_.global_last_executed)
      return commit_rpc_response{last_executed_: log_.last_executed()}

    - commit_thread()
      while (running_)
        wait_until_leader()
        # this peer is a leader now
        #
        # initialize global_last_executed and update it later on each iteration;
        # we send a commit to ourselves as well, which results in trimming of
        # our log.
        global_last_executed = log_.global_last_executed()
        while (running_)
          commit_num_rpcs_ = 0
          commit_responses_ = []

          mu_.lock()
          commit_request_.set_ballot(ballot_)
          mu_.unlock
          commit_request_.set_last_executed(log_.last_executed())
          commit_request_.set_global_last_executed(global_last_executed)

          for each peer p
            post a closure to a thread pool {
              status = p.Commit(commit_request_)
              commit_mu_.lock()
              ++commit_num_rpcs_
              if status.ok
                commit_responses_.push_back(status.response)
              commit_cv_.notify_one()
            }
            commit_mu_.unlock()

          commit_mu_.lock()
          # for commits, we have to wait until we have sent rpcs to all of
          # the peers; this is unlike accept or prepare rpcs because with
          # commits we receive last_executed from peers, and if we want to
          # advance global_last_executed, we need to receive last_executed from
          # all peers so that we could be sure the new global_last_executed is a
          # global minimum.
          while (is_leader() && commit_num_rpcs_ != peers_.size())
            commit_cv_.wait(hb_mu)
          if commit_responses_.size() == peer_.size()
            global_last_executed = min(ok_responses)
          commit_mu_.unlock()

          sleep(commit_interval_)
          if !is_leader()
            break

    - prepare_handler(request: prepare_rpc_request)
      # description: handler for the prepare RPC
      # preconditions:
      # postconditions:

      mu_.lock() # unlocks on return
      if request.ballot_ >= ballot_
        ballot_ = request_.ballot_
        return prepare_rpc_response{type_: ok,
                                    ballot_: N/A,
                                    log_: log_.instances_since_global_last_executed()}
      # reject stale RPC requests
      return prepare_rpc_response{type_: reject, ballot_: ballot_, log_: N/A}


    - prepare() -> prepare_result_t:
      num_responses = 0
      ok_logs = vector<instance_t[]>
      cv, mu
      request = prepare_rpc_request{ballot_: next_ballot#()}
      for each peer p {
        run closure in a separate thread {
          response = p.prepareRPC(request)
          lock(mu)
          ++num_responses
          if response.type_ == ok:
            ok_logs.push(response.log_)
          else if response.type_ == reject:
            ballot_ = response.ballot_
          # else it is a timeout error; we do nothing
          unlock(mu)
          cv.notify_one()
        }
      }
      lock(mu)
      while i_am_leader() &&
            num_ok_responses <= peers_.size()/2 &&
            num_responses != peers_.size()
        cv.wait(mu)
      # one of the above three conditions is false; handle each, starting with the
      # most likely one
      if num_ok_responses > peers_.size()/2: # we have quorum
        return prepare_result_t{type_: ok, log_: ok_logs}
      if someone_else_is_leader():
        return prepare_result_t{type_: reject}
      # multiple timeout responses
      return prepare_result_t{type_: timeout}

    - prepare_thread()
      for (;;) {
        sleep until follower
        for (;;) {
          sleep(commit_interval_ + random(10, commit_interval_))
          if time::now() - last_commit_ < commit_interval_:
            continue
          prepare_result_t result = prepare()
          if result.type_ != ok:
            continue
          # we are a leader
          wake up commit_thread
          ready_ = false
          log_.merge(result.logs_)
          if (replay())
            ready_ = true
          break
        }
      }

    - accept_handler(request: accept_rpc_request)
      # description: handler for the accept RPC
      # preconditions:
      # postconditions:

      if request.ballot_ >= ballot_:
        ballot_ = request.ballot_
        log_.append(request.instance_)
        return accept_rpc_response{type_: ok, ballot_: N/A}
      # stale message
      return accept_rpc_response{type_: reject, ballot_: ballot_}

    - accept(cmd: command, index: int, client-id: int) -> accept_result_t
      num_responses = 0
      num_ok_responses = 0
      cv, mu
      request = accept_rpc_request{command_: cmd,
                                   index_: index,
                                   ballot_: ballot_,
                                   client-id_: client-id}
      for each peer p {
        run closure in a separate thread {
          response = p.acceptRPC(request)
          lock(mu)
          ++num_responses
          if response.type_ == ok:
            ++ok_responses
          else if response.type_ == reject:
            ballot_ = response.ballot_
          # else it is a timeout error; we do nothing
          unlock(mu)
          cv.notify_one()
        }
      }
      lock(mu)
      while i_am_leader() &&
            num_ok_responses <= peers_.size()/2 &&
            num_responses != peers_.size():
        cv.wait(mu)

      if num_ok_responses > peers_.size() / 2
        log_.commit(index)
        return accept_result_t{type_: ok, leader_: N/A}
      if someone_else_is_leader()
        return accept_result_t{type_: reject, leader_: leader()}
      # RPCs timed out
      return accept_result_t{type_: retry, leader_: N/A}

    - replay() -> bool
      for i in log_.new_instances()
        accept_result_t r = accept(i.command_, i.index, i.client-id_)
        if r.type_ == leader
          return false
        if r.type_ == retry
          continue
      return true


- unit tests

  - constructor
    - check that class members are initialized correctly.

  - next_ballot
    - check that unique and higher ballot numbers are generated by different
      paxos peers.

  - requests_with_lower_ballot_ignored
    - check that stale rpcs (those with older than peer's ballot numbers) are
      ignored by the commit rpc handler:

      - in thread t0 start peer0 rpc server
      - in thread main, call peer0.next_ballot twice
      - in thread main, create an rpc request and set its ballot to the
        result of calling peer1.next_ballot
      - in thread main, invoke prepare, accept, commit rpcs on peer0

      since peer1.next_ballot was called once, the ballot on the RPCs should be
      smaller than the ballot on peer0; therefore, the rpcs should be ignored
      and peer0 should remain the leader.

  - requests_with_higher_ballot_change_leader_to_follower
    - start peer0 as a leader, and check that an rpc with higher than peer0's
      ballot number from peer1 changes the peer0 to follower and that peer0
      considers peer1 to be the leader.

      - in thread t0, start peer0 rpc server
      - in thread main, call peer0_.next_ballot to make peer0 leader
      - in thread main, create an rpc request and set its ballot to the result
        of calling peer1.next_ballot -- now peer1 is a leader with a higher
        ballot number than peer0, but peer0 does not yet know it.
      - in thread main, send the rpc request to peer0
      - check that peer0 is not a leader anymore and considers peer1 as the
        leader.
      - repeat the above for prepare, accept, and commit

  - commit_commits_and_trims
    - append to peer0's log three in-progress instances with indexes 1, 2, and 3
    - send a commit rpc to peer0 with last_executed = 2 and
      global_last_executed = 0
    - check that the last_executed in response is 0 and instances 1 and 2 are in
      committed state and instance 3 is in in-progress state
    - execute instances 1 and 2
    - send another commit rpc to peer0 with last_executed = 2 and
      global_last_executed = 2
    - check that the last_executed in response is 2 and instances 1 and 2 are
      trimmed

  - prepare_responsds_with_correct_instances
    - append to peer0's log three in-progress instances with indexes 1, 2, and 3
    - send a prepare rpc to peer0 and check that the response contains instances
      1, 2, and 3
    - send a commit rpc to peer0 with last_executed = 2 and
      global_last_executed = 0
    - instances 1 and 2 should be committed now; execute them
    - send a prepare rpc to peer0 and check that the response contains instances
      1 and 2 in executed state and instance 3 in in-progress state
    - send another commit rpc to peer0 with last_executed = 2 and
      global_last_executed = 2; peer0 should trim instances 1 and 2
    - send a prepare rpc to peer0 and check that the response contains only the
      instance 3

  - accept_appends_to_log
    - send an accept rpc to peer0 with an instance at index 1 and check that
      peer0's log contains the instance
    - send an accept rpc to peer0 with an instance at index 2 and check that
      peer0's log contains both instances

  - prepare_response_with_higher_ballot_changes_leader_to_follower
    - call peer0_.next_ballot to make it a leader
    - call peer1_.next_ballot to make it a leader
    - call peer2_.next_ballot to make it a leader
    - send a commit with peer2_'s ballot to peer1_ to establish peer2 as a
      leader in peer1 as well
    - call run_prepare_phase from peer0_ and expect it to become a follower
      assume peer2 as the leader

  - accept_response_with_higher_ballot_changes_leader_to_follower
    - call peer0_.next_ballot to make it a leader
    - call peer1_.next_ballot to make it a leader
    - call peer2_.next_ballot to make it a leader
    - send a commit with peer2_'s ballot to peer1_ to establish peer2 as a
      leader in peer1 as well
    - call run_accept_phase from peer0_ and expect it to become a follower
      assume peer2 as the leader

  - commit_response_with_higher_ballot_changes_leader_to_follower
    - call peer0_.next_ballot to make it a leader
    - call peer1_.next_ballot to make it a leader
    - call peer2_.next_ballot to make it a leader
    - send a commit with peer2_'s ballot to peer1_ to establish peer2 as a
      leader in peer1 as well
    - call run_commit_phase from peer0_ and expect it to become a follower
      assume peer2 as the leader

  - run_prepare_phase
    - check that run_prepare_phase returns null when it hasn't received
      responses from quorum, and it returns a proper log when it has heard from
      the quorum.
      - append to peer0, peer1 logs an instance at index 1
      - start peer0 rpc server
      - call run_prepare_phase and expect null as the response
      - start peer1 rpc server
      - call run_prepare_phase and expect a response with a log containing the
        instance

  - run_accept_phase
    - check that run_accept_phase returns retry when it hasn't heard back from
      the quorum, and it returns ok otherwise
      - start peer0 rpc server
      - call peer0.run_accept_phase and expect retry as the response and peer0's
        loghas the instance at index 1 in in-progress state and peer1 and peer2
        have no entries at index 1
      - start peer1 rpc server
      - call peer0.run_accept_phase and expect ok as the response and peer0's
        log has the instance at index 1 in committed state and peer1 in
        in-progress state and peer2 has no entry at index 1.

  - run_commit_phase
    - check that run_commit_phase returns the passed in global_last_executed
      when it hasn't heard from all of the peers, and it returns the new
      global_last_executed when it has heard from all of the peers.
      - start peer0 and peer1 rpc servers
      - append to peer0's log committed instances at index 1, 2, and 3, and
        execute all instances
      - append to peer1's log committed instances at index 1, 2, and 3, and
        execute all instances
      - append to peer2's log committed instances at index 1 and 2, and execute
        all instances
      - call run_commit_phase with global_last_executed = 0 and expect 0 as the
        response
      - start peer2 rpc server
      - append to peer2 an in-progress instance at index 3
      - call run_commit_phase with global_last_executed = 0 and expect 2 as the
        response
      - peer2 should have now committed the instance at index3; execute it
      - call run_commit_phase with global_last_executed = 2 expect 3 as the
        response

  - one_leader_elected
    - start peer0, peer1, peer2, wait for 3x of commit interval and then
      check that a single leader is elected.

- MultiPaxos
  # The Paxos module that contains the consensus logic for phase1 and phase2 of
  # the MultiPaxos protocol.

  - members

    - running_: atomic<bool>; a flag used by long-running threads (heartbeat
      thread and prepare thread) to know when to stop running; initialized to
      false, set to true with the start() method and to false with the
      shutdown() method.

    - ready_: bool; a flag that indicates if the peer---after becoming a
      leader---is ready to respond to client requests. we need this flag because
      as soon as this peer calls next_ballot() in prepare_thread(), it becomes a
      leader, but it may not yet be able to serve client requests since it may
      need to recover the log and replay the instances it received as a response
      to prepare request, to catch up with the rest of the peers; during this
      replay, the ready_ flag is false and this peer responds to the clients
      with "retry"; as soon as this peer has caught up, we set ready_ to true
      and start responding to the clients. initialized to false; set to false in
      next_ballot() and set to true once replay has completed.

    - id_: int64_t; identifier of this peer. initialized from the configuration
      file. currently, we limit the number of peers to 16; therefore, id_ is a
      value in [0, 16).

    - port_: string; the port where the RPC server is listening

    - ballot_: int64_t; current ballot number known to the peer;
      initialized |max_num_peers_|, which indicates that there is no current
      leader, since valid leader ids are in [0, max_num_peers_). it is a 64-bit
      integer, the lower 8 bits of which is always equal to |id_| of the peer
      that chose |ballot_|, and the higher bits represent the round number. we
      preserve 8 bits for |id_| but limit |id_| to 4 bits to avoid an overflow
      when identifying a leader. initialized to |id_|. at any moment, we can
      look at the lower 8 bits of |ballot_| to determine current leader.

    - log_: Log; a non-owning pointer to Log instance.

    - peers_: array of RPC endpoints; RPC endpoints to peers, including to self.
      initialized from the configuration file.

    - ready_: boolean; when a new leader is elected, it receives logs from the
      peers in the quorum; the leader merges all these logs to obtain the most
      up-to-date instance at each index in the log and then replays (calls
      accept on) all the instances in the merged log. for now, we consider the
      leader not ready to accept new requests from the clients until it has
      processed the merged log; the |ready_| flag tracks the readiness for
      accepting commands from clients. initialized to false.

    - last_heartbeat_: timestamp; timestamp of the last heartbeat received from the current
      leader. initialized to 0.

    - heartbeat_interval_: milliseconds; time between two consecutive
      heartbeats. initialized from the configuration file.

    - rpc_peers_: stubs to RPC peers; initialized based on the configuration file

    - rpc_server_: an RPC server for handling incoming RPC requests; initialized
      based on the configuration file

    - mu_: mutex; MultiPaxos is a concurrent object -- multiple threads may
      concurrently call its |decide| method. |mu_| protects shared data in
      MultiPaxos.

    - tp_: thread_pool; a thread pool to which we post RPC requests to peers;
      initialized from the configuration file.

    # the followings are heartbeat thread related variables. we cannot make them
    # local to the heartbeat thread because the heartbeat thread posts tasks to
    # the threadpool tp_; it is possible that the threads from the threadpool
    # outlive the heartbeat thread, and if these variables are local to the
    # heartbeat thread, then the threadpool threads will end up referencing
    # destroyed variables. One other possible solution is to have the threadpool
    # local to the heartbeat thread as well.

    - heartbeat_thread_: thread; runs the function heartbeat_thread() until
      shutdown() is called.

    - cv_leader_: condition variable; heartbeat_thread sleeps on this condition
      variable to get notified when this peer becomes a leader.

    - heartbeat_request_: heartbeat RPC request protobuf; initialized every time
      right before heartbeats are sent by acquiring mu_ and getting a consistent
      set of its fields (ballot, last_executed, and global_last_executed) from
      this peer.

    - heartbeat_num_rpcs_: int; the number of heartbeat rpcs sent; initialized
      to 0 every time right before heartbeats are sent.

    - heartbeat_responses_: array of ints; the values of responses to
      heartbeats; reset to empty every time right before heartbeats are sent.

    - heartbeat_mu_: mutex; serializes access to heartbeat_num_responses_ and
      heartbeat_ok_responses_ among thread pool threads and the heartbeat
      thread.

    - heartbeat_cv_: condition variable; the heartbeat thread waits on this
      condition variable and it is woken up every time an RPC posted by a
      threadpool thread completes and signals this condition variable.

    # the followings are prepare thread related variables. we cannot make them
    # local to the prepare thread for the same reason as above, with the
    # heartbeat thread.

    - prepare_thread_: thread; runs the function prepare_thread() until
      shutdown() is called.

    - cv_follower_: condition variable; prepare_thread sleeps on this condition
      variable to get notified when this peer becomes a follower.

    - prepare_num_rpcs_: int; the number of prepare rpcs sent; initialized to 0
      every time right before prepare rpcs are sent.

    - prepare_ok_responses_: array of log_vector_t; log_vector_ts received in a
      round of prepare rpcs

  - constants

    - id_bits_ = 0xff: the lower bits of |ballot_| we use for storing the id of
      the current leader.

    - round_increment_ = id_bits_ + 1: we add this constant to |ballot_| to
      increment the round portion of |ballot_| by one.

    - max_num_peers_ = 0xf: maximum number of peers.

  - types

    - heartbeat_rpc_request
      - ballot_: ballot of the leader sending the heartbeat RPC
      - last_executed_: last_executed_ of the leader sending the heartbeat RPC
      - global_last_executed_: global_last_executed of the leader sending the heartbeat RPC
      - sender: id of the sender

    - heartbeat_rpc_response
      - last_executed_: last_executed_ of the follower responding to the
        heartbeat RPC

    - prepare_rpc_request
      - ballot_: ballot of the sender
      - sender: id of the sender

    - prepare_rpc_response
      - type_: enum {ok, reject}
      - ballot_: ballot of the peer (valid only if type_ == reject)
      - instances_: Instances of the peer since global_last_executed_ (valid
        only if type_ == ok)

    - accept_rpc_request
      - instance: Instance sent by the leader
      - sender: id of the sender

    - accept_rpc_response
      - type_: enum {ok, reject}
      - ballot_: ballot of the peer (valid only if type_ == reject)

  - methods

    - constructor(log: *Log, cfg: config)
      running_ = false
      id_ = config["id"]
      port_ = config["peers"][id_]
      ballot_ = max_num_peers_
      log_ = log
      instantiate RPC stubs to each peer including self
      instantiate RPC server

    - start()
      # starts all the threads of the peer: RPC server threads, heartbeat
      # threads, prepare threads. a multipaxos instance on which start() was
      # called, must have a corresponding shutdown() call for a graceful
      # shutdown; conversely, if start() wasn't called, then shutdown() must not
      # be called.
      running_ = true
      heartbeat_thread_ = thread(heartbeat_thread()) # start heartbeat thread
      rpc_server_->wait() # start the RPC server so it listens for incoming calls

    - shutdown()
      running_ = false
      cv_leader_.notify_all()
      heartbeat_thread_.join()
      rpc_server_->shutdown() # stop the RPC server

    - next_ballot() -> int
      # description: gets the next ballot number by incrementing the round
      # portion of |ballot_| by |round_increment_| and setting the |id_| bits to
      # the id if this peer, since |ballot_| could have been generated by
      # another peer. calling this method makes this peer the leader, which is
      # why we also signal the cv_leader_ condition variable to wake up the
      # long-running threads, heartbeat_thread and prepare_thread.
      # preconditions:
      # postconditions:
      mu_.lock() # unlocks on return
      ballot_ += id_bits_
      ballot_ = (ballot_ & ~id_bits_) | id_
      cv_leader_.notify_all()
      return ballot_

    - leader() -> int
      # description: returns current leader's id
      # preconditions:
      # postconditions:
      mu_.lock() # unlocks on return
      return ballot_ & id_bits_

    - is_leader() -> bool
      # description: returns true if this peer is leader
      # preconditions:
      # postconditions:
      mu_.lock() # unlocks on return
      return is_leader_lockless()

    - is_leader_lockless() -> bool
      # description: returns true if this peer is leader
      # preconditions: lock is not acquired
      # postconditions:
      return (ballot_ & id_bits) == id_

    - is_someone_else_leader() -> bool
      # description: returns true if we are not a leader and someone else is.
      # this cannot simply be !is_leader() because it's not necessarily the case
      # that if this peer is not leader, then someone else must be: at startup
      # there is no leader.
      # preconditions:
      # postconditions:
      mu_.lock() # unlocks on return
      id = ballot_ & id_bits_
      return id != id_ && id < max_num_peers_

    - wait_until_leader()
      # description: waits until this peer becomes a leader
      # preconditions:
      # postconditions:
      mu_.lock()
      while running_ && !is_leader_lockless()
        cv_leader_.wait(mu_)
      mu_.unlock()

    - replicate(cmd: command, client-id: int) -> accept_result_t
      if i_am_leader()
        if ready_
          return accept(cmd, log_.advance_last_index(), client-id)
        return accept_result_t{type_: retry, leader_: N/A}
      if someone_else_is_leader()
        return accept_result_t{type_: reject, leader_: leader()}
      # election in progress
      return accept_result_t{type_: retry, leader_: N/A}

    - heartbeat_handler(request: heartbeat_rpc_request)
      # description: handler for the heartbeat RPC
      # preconditions:
      # postconditions:

      mu_.lock() # unlocks on return
      if request.ballot_ >= ballot_
        last_heartbeat_ = time::now()
        ballot_ = request.ballot_
        stale_rpc = false
        log_.commit_until(request_.last_executed_, ballot_)
        log_.trim_until(request_.global_last_executed)
      return heartbeat_rpc_response{last_executed_: log_.last_executed()}

    - heartbeat_thread()
      while (running_)
        wait_until_leader()
        # this peer is a leader now
        #
        # initialize global_last_executed and update it later on each iteration;
        # we send a heartbeat to ourselves as well, which results in trimming of
        # our log.
        global_last_executed = log_.global_last_executed()
        while (running_)
          heartbeat_num_rpcs_ = 0
          heartbeat_responses_ = []

          mu_.lock()
          heartbeat_request_.set_ballot(ballot_)
          mu_.unlock
          heartbeat_request_.set_last_executed(log_.last_executed())
          heartbeat_request_.set_global_last_executed(global_last_executed)

          for each peer p
            post a closure to a thread pool {
              status = p.Heartbeat(heartbeat_request_)
              heartbeat_mu_.lock()
              ++heartbeat_num_rpcs_
              if status.ok
                heartbeat_responses_.push_back(status.response)
              heartbeat_cv_.notify_one()
            }
            heartbeat_mu_.unlock()

          heartbeat_mu_.lock()
          # for heartbeats, we have to wait until we have sent rpcs to all of
          # the peers; this is unlike accept or prepare rpcs because with
          # heartbeats we receive last_executed from peers, and if we want to
          # advance global_last_executed, we need to receive last_executed from
          # all peers so that we could be sure the new global_last_executed is a
          # global minimum.
          while (is_leader() && heartbeat_num_rpcs_ != peers_.size())
            heartbeat_cv_.wait(hb_mu)
          if heartbeat_responses_.size() == peer_.size()
            global_last_executed = min(ok_responses)
          heartbeat_mu_.unlock()

          sleep(heartbeat_interval_)
          if !is_leader()
            break

    - prepare_handler(request: prepare_rpc_request)
      # description: handler for the prepare RPC
      # preconditions:
      # postconditions:

      mu_.lock() # unlocks on return
      if request.ballot_ >= ballot_
        ballot_ = request_.ballot_
        return prepare_rpc_response{type_: ok,
                                    ballot_: N/A,
                                    log_: log_.instances_since_global_last_executed()}
      # reject stale RPC requests
      return prepare_rpc_response{type_: reject, ballot_: ballot_, log_: N/A}


    - prepare() -> prepare_result_t:
      num_responses = 0
      ok_logs = vector<instance_t[]>
      cv, mu
      request = prepare_rpc_request{ballot_: next_ballot#()}
      for each peer p {
        run closure in a separate thread {
          response = p.prepareRPC(request)
          lock(mu)
          ++num_responses
          if response.type_ == ok:
            ok_logs.push(response.log_)
          else if response.type_ == reject:
            ballot_ = response.ballot_
          # else it is a timeout error; we do nothing
          unlock(mu)
          cv.notify_one()
        }
      }
      lock(mu)
      while i_am_leader() &&
            num_ok_responses <= peers_.size()/2 &&
            num_responses != peers_.size()
        cv.wait(mu)
      # one of the above three conditions is false; handle each, starting with the
      # most likely one
      if num_ok_responses > peers_.size()/2: # we have quorum
        return prepare_result_t{type_: ok, log_: ok_logs}
      if someone_else_is_leader():
        return prepare_result_t{type_: reject}
      # multiple timeout responses
      return prepare_result_t{type_: timeout}

    - prepare_thread()
      for (;;) {
        sleep until follower
        for (;;) {
          sleep(heartbeat_interval_ + random(10, heartbeat_interval_))
          if time::now() - last_heartbeat_ < heartbeat_interval_:
            continue
          prepare_result_t result = prepare()
          if result.type_ != ok:
            continue
          # we are a leader
          wake up heartbeat_thread
          ready_ = false
          log_.merge(result.logs_)
          if (replay())
            ready_ = true
          break
        }
      }

    - accept_handler(request: accept_rpc_request)
      # description: handler for the accept RPC
      # preconditions:
      # postconditions:

      if request.ballot_ >= ballot_:
        ballot_ = request.ballot_
        log_.append(request.instance_)
        return accept_rpc_response{type_: ok, ballot_: N/A}
      # stale message
      return accept_rpc_response{type_: reject, ballot_: ballot_}

    - accept(cmd: command, index: int, client-id: int) -> accept_result_t
      num_responses = 0
      num_ok_responses = 0
      cv, mu
      request = accept_rpc_request{command_: cmd,
                                   index_: index,
                                   ballot_: ballot_,
                                   client-id_: client-id}
      for each peer p {
        run closure in a separate thread {
          response = p.acceptRPC(request)
          lock(mu)
          ++num_responses
          if response.type_ == ok:
            ++ok_responses
          else if response.type_ == reject:
            ballot_ = response.ballot_
          # else it is a timeout error; we do nothing
          unlock(mu)
          cv.notify_one()
        }
      }
      lock(mu)
      while i_am_leader() &&
            num_ok_responses <= peers_.size()/2 &&
            num_responses != peers_.size():
        cv.wait(mu)

      if num_ok_responses > peers_.size() / 2
        log_.commit(index)
        return accept_result_t{type_: ok, leader_: N/A}
      if someone_else_is_leader()
        return accept_result_t{type_: reject, leader_: leader()}
      # RPCs timed out
      return accept_result_t{type_: retry, leader_: N/A}

    - replay() -> bool
      for i in log_.new_instances()
        accept_result_t r = accept(i.command_, i.index, i.client-id_)
        if r.type_ == leader
          return false
        if r.type_ == retry
          continue
      return true


- unit tests

  - constructor
    - ensure that class members are initialized correctly.

  - next_ballot
    - ensure that unique and higher ballot numbers are generated by different
      paxos peers.

  - requests_with_lower_ballot_ignored
    - ensure that stale RPCs (those with older than peer's ballot numbers) are
      ignored by the heartbeat RPC handler:

      1) in thread t0 start peer0 RPC server
      2) in thread main, call peer0.next_ballot twice
      3) in thread main, create an RPC request and set its ballot to the
         result of calling peer1.next_ballot
      4) in thread main, invoke heartbeat, prepare, accept RPCs on peer0

      since peer1.next_ballot was called once, the ballot on the RPCs should be
      smaller than the ballot on peer0; therefore, the RPCs should be ignored
      and peer0 should remain the leader.

  - requests_with_higher_ballot_change_leader_to_follower
    - start peer0 as a leader, and ensure that an RPC with higher than peer0's
      ballot number from peer1 changes the peer0 to follower and that peer0
      considers peer1 to be the leader.

      1) in thread t0, start peer0 RPC server
      2) in thread main, call peer0_.next_ballot to make peer0 leader
      3) in thread main, create an RPC request and set its ballot to the result
         of calling peer1.next_ballot -- now peer1 is a leader with a higher
         ballot number than peer0, but peer0 does not yet know it.
      4) in thread main, send request0 to peer0
      5) ensure that peer0 is not a leader anymore and considers peer1 as the leader.
      6) repeat the above for heartbeat, prepare, and accept requests

  - heartbeat_commits_and_trims
    - append to log0 three in-progress instances with indices 1, 2, and 3
    - instantiate peer0 with log0
    - send a heartbeat rpc to peer0 with last_executed = 2 and
      global_last_executed = 0
    - ensure that the last_executed in response is 0 and instances 1 and 2 are
      in committed state
    - execute instances 1 and 2 in the log
    - send another heartbeat rpc to peer0 with last_executed = 2 and
      global_last_executed = 2
    - ensure that the last_executed in response is 2 and instances 1 and 2 are
      trimmed

  - prepare_responsds_with_correct_instances
    - append to log0 three in-progress instances with indices 1, 2, and 3
    - instantiate peer0 with log0
    - send a prepare rpc to peer0 and ensure that the response contains
      instances 1, 2, and 3
    - send a heartbeat rpc to peer0 with last_executed = 2 and
      global_last_executed = 0
    - execute instances 1 and 2 in the log
    - send a prepare rpc to peer0 and ensure that the response contains
      instances 1 and 2 in executed state and instance 3 in in-progress state
    - send another heartbeat rpc to peer0 with last_executed = 2 and
      global_last_executed = 2
    - send a prepare rpc to peer0 and ensure that the response contains only the
      instance 3

  - accept_appends_to_log
    - send an accept rpc to peer0 and ensure that instance is appended to log.

  - heartbeat_response_with_high_ballot_changes_leader_to_follower
    - call peer0_.next_ballot to make it a leader
    - call peer1_.next_ballot to make it a leader
    - call peer2_.next_ballot to make it a leader
    - send a heartbeat with peer2_'s ballot to peer1_ to establish peer2 as a
      leader in peer1 as well
    - call send_heartbeats from peer0_ and expect it to become a follower assume
      peer2 as the leader

  - prepare_response_with_high_ballot_changes_leader_to_follower
    - call peer0_.next_ballot to make it a leader
    - call peer1_.next_ballot to make it a leader
    - call peer2_.next_ballot to make it a leader
    - send a heartbeat with peer2_'s ballot to peer1_ to establish peer2 as a
      leader in peer1 as well
    - call send_prepares from peer0_ and expect it to become a follower assume
      peer2 as the leader

  - accept_response_with_high_ballot_changes_leader_to_follower
    - call peer0_.next_ballot to make it a leader
    - call peer1_.next_ballot to make it a leader
    - call peer2_.next_ballot to make it a leader
    - send a heartbeat with peer2_'s ballot to peer1_ to establish peer2 as a
      leader in peer1 as well
    - call send_accepts from peer0_ and expect it to become a follower assume
      peer2 as the leader

  - send_heartbeats
    - ensure that send_heartbeats works correctly; specifically, it returns the
      passed in global_last_executed when it hasn't heard from all of the peers,
      and it returns the new global_last_executed when it has heard from all of
      the peers.
      - append to peer0, peer1, peer2 logs an instance at index 1
      - commit and execute the instance in all peers' logs
      - start peer0 and peer1 rpc servers
      - call send_heartbeats with global_last_executed = 0 and expect 0 as the response
      - start peer2 rpc server
      - call send_heartbeats with global_last_executed = 0 and expect 1 as the response

  - one_leader_elected
    - start peer0, peer1, peer2, wait for 3x of heartbeat interval and then
      check that a single leader is elected.

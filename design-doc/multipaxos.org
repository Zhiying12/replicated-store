- MultiPaxos
  # The Paxos module that contains the consensus logic for phase1 and phase2 of
  # the MultiPaxos protocol.

  - members

    - running_: atomic<bool>; a flag used by long-running threads (heartbeat
      thread and prepare thread) to know when to stop running; initialized to
      false, set to true with the start() method and to false with the
      shutdown() method.

    - id_: int64_t; identifier of this peer. initialized from the configuration
      file. currently, we limit the number of peers to 16; therefore, id_ is a
      value in [0, 16).

    - port_: string; the port where the RPC server is listening

    - ballot_: int64_t; current ballot number known to the peer;
      initialized |max_num_peers_|, which indicates that there is no current
      leader, since valid leader ids are in [0, max_num_peers_). it is a 64-bit
      integer, the lower 8 bits of which is always equal to |id_| of the peer
      that chose |ballot_|, and the higher bits represent the round number. we
      preserve 8 bits for |id_| but limit |id_| to 4 bits to avoid an overflow
      when identifying a leader. initialized to |id_|. at any moment, we can
      look at the lower 8 bits of |ballot_| to determine current leader.

    - log_: Log; a non-owning pointer to Log instance.

    - peers_: array of RPC endpoints; RPC endpoints to peers, including to self.
      initialized from the configuration file.

    - ready_: boolean; when a new leader is elected, it receives logs from the
      peers in the quorum; the leader merges all these logs to obtain the most
      up-to-date instance at each index in the log and then replays (calls
      accept on) all the instances in the merged log. for now, we consider the
      leader not ready to accept new requests from the clients until it has
      processed the merged log; the |ready_| flag tracks the readiness for
      accepting commands from clients. initialized to false.

    - last_heartbeat_: timestamp; timestamp of the last heartbeat received from the current
      leader. initialized to 0.

    - heartbeat_interval_: milliseconds; interval between heartbeats.
      initialized from the configuration file.

    - rpc_peers_: stubs to RPC peers; initialized based on the configuration file

    - rpc_server_: an RPC server for handling incoming RPC requests; initialized
      based on the configuration file

    - mu_: mutex; MultiPaxos is a concurrent object -- multiple threads may
      concurrently call its |decide| method. |mu_| protects shared data in
      MultiPaxos.

  - constants

    - id_bits_ = 0xff: the lower bits of |ballot_| we use for storing the id of
      the current leader.

    - round_increment_ = id_bits_ + 1: we add this constant to |ballot_| to
      increment the round portion of |ballot_| by one.

    - max_num_peers_ = 0xf: maximum number of peers.

  - types

    - heartbeat_rpc_request
      - ballot_: ballot of the leader sending the heartbeat RPC
      - last_executed_: last_executed_ of the leader sending the heartbeat RPC
      - global_last_executed_: global_last_executed of the leader sending the heartbeat RPC
    - heartbeat_rpc_response
      - last_executed_: last_executed_ of the follower responding to the
        heartbeat RPC

  - methods

    - constructor(log: *Log, cfg: config)
      running_ = false
      id_ = config["id"]
      port_ = config["peers"][id_]
      ballot_ = max_num_peers_
      log_ = log
      instantiate RPC stubs to each peer including self
      instantiate RPC server

    - start()
      running_ = true
      rpc_server_->wait() # start the RPC server so it listens for incoming calls

    - shutdown()
      running_ = false
      rpc_server_->shutdown() # stop the RPC server

    - next_ballot() -> int
      # description: gets the next ballot number by incrementing the round
      # portion of |ballot_| by |round_increment_| and setting the |id_| bits to
      # the id if this peer, since |ballot_| could have been generated by
      # another peer. calling this method makes this peer the leader.
      # preconditions:
      # postconditions:
      mu_.lock()
      ballot_ += id_bits_
      ballot_ = (ballot_ & ~id_bits_) | id_
      return ballot_

    - leader() -> int
      # description: returns current leader's id
      # preconditions:
      # postconditions:
      mu_.lock()
      return ballot_ & id_bits_

    - is_leader() -> bool
      # description: returns true if this peer is leader
      # preconditions:
      # postconditions:
      mu_.lock()
      return leader() == id_

    - is_someone_else_leader() -> bool
      # description: returns true if we are not a leader and someone else is.
      # this cannot simply be !is_leader() because it's not necessarily the case
      # that if this peer is not leader, then someone else must be: at startup
      # there is no leader.
      # preconditions:
      # postconditions:
      mu_.lock()
      id = ballot_ & id_bits_
      return id != id_ && id < max_num_peers_

    - heartbeat_handler(request: heartbeat_rpc_request)
      # description: handler for the heartbeat RPC
      # preconditions:
      # postconditions:

      # a leader does not send heartbeats to itself; therefore, it should never
      # receive a ballot in a heartbeat message that has the same id bits as its
      # own (i.e. a ballot that was originated by this peer).
      assert(!is_leader() || id_ != request.ballot_ & id_bits_) << "heartbeat case 1"

      stale_rpc = true
      mu_.lock()
      if message.ballot_ >= ballot_:
        last_heartbeat_ = time::now()
        ballot_ = message.ballot_
        stale_rpc = false
      mu_.unlock()

      # our log object is concurrent, so we don't need to hold the lock during
      # the whole RPC handling. instead, we introduce stale_rpc boolean that
      # is set to false when the lock is held and the ballots are compared.
      if !stale_rpc:
        log_.commit_until(message_.last_executed_, ballot_)
        log_.trim_until(message_.global_last_executed)
      return heartbeat_rpc_response{last_executed_: log_.last_executed()}

- unit tests

  - constructor
    - ensure that class members are initialized correctly.

  - next_ballot
    - ensure that unique and higher ballot numbers are generated by different
      paxos peers.

  - heartbeat_ignore_stale_rpc
    - ensure that stale RPCs (those with older than peer's ballot numbers) are
      ignored by the heartbeat RPC handler:

      1) in thread t0 start peer0 RPC server
      2) in thread main, call peer0.next_ballot twice
      3) in thread main, create a heartbeat RPC request and set its ballot to the
         result of calling peer1.next_ballot
      4) in thread main, invoke heartbeat RPC on peer0

      since peer1.next_ballot was called once, the ballot on the heartbeat RPC
      should be smaller than the ballot on peer0; therefore, the RPC request
      should be ignored and peer0 should remain the leader.

  - heartbeat_invalid_ballot (death test) *THIS TEST IS NOT IMPLEMENTED*
    - a death test for "heartbeat case 1" above

      1) in thread t1, start peer0 RPC server
      2) in thread t2, create a heartbeat RPC request and set its ballot to the
         result of calling peer0.next_ballot

      this should trigger the assertion "heartbeat case 1"

  - heartbeat_changes_leader_to_follower
    - start peer0 as a leader, and ensure that a valid RPC (those with higher
      than peer's ballot numbers) from peer1 changes the peer0 to follower and
      that peer0 considers peer1 to be the leader.

      1) in thread t0, start peer0 RPC server
      2) in thread main, call peer0_.next_ballot to make peer0 leader
      3) in thread main, create a heartbeat RPC request request0 and set its
         ballot to the result of calling peer1.next_ballot -- now peer1 is a
         leader with a higher ballot number than peer0, but peer0 does not yet
         know it.
      4) in thread main, send request0 to peer0
      5) ensure that peer0 is not a leader anymore and considers peer1 as the leader.

  - heartbeat_updates_leader_on_followers
    - start peer0 and peer1 leaders, send an RPC from peer2 to demote them to
      followers and ensure that they know of the new leader; then send another
      RPC from peer3 and ensure that they know of the newer leader

      1) in thread t0, start peer0 RPC server
      2) in thread t1, start peer1 RPC server
      3) in thread main, call peer0_.next_ballot and peer1_.next_ballot to make
         them leaders
      4) in thread main, create a heartbeat RPC request and set its ballot to
         the result of calling peer2.next_ballot
      5) in thread main, send request created in step (4) to peer0 and peer1
      6) ensure that peer0 and peer1 are not leaders anymore and they correctly
         know that peer2 is the new leader
      7) in thread main, create a heartbeat RPC request and set its ballot to
         the result of calling peer3.next_ballot
      8) in thread main, send request created in step (7) to peer0 and peer1
      9) ensure that peer0 and peer1 are not leaders and they correctly know
         that peer3 is the new leader

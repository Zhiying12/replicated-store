== TYPES =======================================================================

- struct command_t
  - synopsis: a key-value command
  - fields:
    - type_: enum {get, put, del}
    - key_: string
    - value_: string (valid only if type_ == put)

- struct result_t
  - synopsis: result of executing a key-value command
  - fields:
    - type_: enum {ok, fail}
    - result_: string (value if type_ == ok, error message otherwise)

- struct instance_t
  - synopsis: entry in the log.
  - fields:
    - round_: round of the instance
    - command_: command_t
    - index_: index of the command in the log
    - state_: enum {in-progress, committed, executed}
  - description: each instance starts in the in-progress state, and changes to
    the committed state once quorum has voted for it, and moves into executed
    state after it is executed. technically, we do not need the executed field
    because we are already tracking it with the tail_ (see below), but we still
    keep it for internal consistency checks: whenever we trim the log, we assert
    that state_ of the trimmed entry is executed.

- struct accept_result_t
  - synopsis: return type of agree_and_execute() and accept()
  - fields:
    - type_: enum {ok, reject, fail, retry}
    - result_: string (if type_ == ok then contains result of command; if type_
      == fail, then contains error message; undefined otherwise)
    - leader_: int (if type_ == reject then contains id of the current leader_)

- message accept_rpc_request
  - command_: command_t
  - index_: int; index for the command_ in log_
  - round_: int; round of the sender

- message accept_rpc_response
  - type_: enum {ok, reject, timeout}
  - round_: round of the remote peer (valid only if type_ == reject)

- struct prepare_result_t
  - synopsis: return type of prepare()
  - fields:
    - type_: enum {ok, reject, timeout}
    - log_: log entries (valid only if type_ == ok)

- message prepare_rpc_request
  - tail_: int
  - round_: int

- message prepare_rpc_response
  - type_: enum {ok, reject, timeout}
  - round_: round of the remote peer (valid only if type_ == reject)
  - log_: instances since tail_ in the corresponding prepareRPC request (valid
    only if type_ == ok}

- message heartbeat_rpc_request
  - min_tail_: int (minimum of all tail_s seen so far)
  - tail_: int (tail of the leader)

- message heartbeat_rpc_response
  - type_: enum {ok, reject, timeout}
  - round_: round of the remote peer (valid only if type_ == reject)
  - tail_: int (local tail_)

== THREADS ACTING ON A PAXOS OBJECT ============================================

- main threads: the entry point of the paxos object is the agree_and_execute
  method. for each incoming command, this method is called in a separate thread.
  hence, at any given time, we may have multiple concurrent threads running
  agree_and_execute method. we call these main threads.

- heartbeat thread: each paxos object runs a heartbeat thread. initially, an
  object starts as a non-leader. when it becomes a leader, it needs to regularly
  notify the other peers to announce its leadership and suppress leader
  election. thus, the heartbeat thread runs an infinite loop, where it sleeps
  until the object becomes a leader, after which it starts sending regular
  heartbeat messages; as soon as it detects that the object is not a leader
  anymore, it goes back to sleep until it is woken up again.

- prepare thread: each paxos object runs a prepare thread for starting election
  (phase1) when there is a need. this thread runs an infinite loop, similar to
  the heartbeat thread, where it sleeps until the object is not a leader
  anymore, after which the object should constantly monitor the incoming
  heartbeats to make sure that the current leader is still alive. whenever it
  detects a dead leader, it runs election---sends out prepare messages---to
  become a leader. at the end of the election either this object or someone else
  is a leader. if it is the leader, then this thread goes back to sleep, until
  this object is not a leader anymore. otherwise, if it is not the leader, the
  thread goes back to monitoring incoming heartbeats to detect when the current
  leader fails.

- request threads: in a main thread, agree_and_execute() calls accept() which
  calls RPCs to peers to get the quorum vote; in prepare thread, prepare() calls
  RPCs to peers to get the quorum vote; the heartbeat thread calls RPCs for
  failure detection. we are currently using blocking RPC calls; therefore, each
  of these calls are made in a separate thread. we call these request threads.

- response threads: the RPC handlers run in threads managed by gRPC runtime. we
  call these response threads.

- execute_and_trim thread: followers will regularly receive heartbeats
  containing leader_tail_ (which instances have been committed and executed at
  the leader) and leader_min_tail_ (which instances have been executed by all of
  the peers). the followers will use this information to execute entries in
  their log and trim the log. one option is do these operations within the
  heartbeat RPC handler thread and respond to the heartbeat after the operations
  have completed. another choice is to do this in a separate thread,
  asynchronously, and let the heartbeat RPC handler respond immediately.
  currently, we do the latter, and we use a separate execute_and_trim thread in
  each peer to perform log trimming and command execution.

== PAXOS OBJECT STATE VARIABLES ================================================

- peers_
  - synopsis: an array of RPC endpoints to the peers, including to self.
  - init: constructed based on the configuration file
  - description: this array is constructed on startup and read when sending
    prepare, accept, and heartbeat messages to the peers.
  - notes:
    - for now, this array is constructed once at the beginning, and from there
      on it is only read from. no need to protect it with a mutex until we start
      supporting reconfiguration, but we should make sure that it is constructed
      completely before starting any threads.
    - assert (len(peers_) <= max_num_peers_); see max_num_peers_ description
      below.

- max_num_peers_:
  - synopsis: a constant representing the maximum number of peers allowed.
  - init: initialized to 16
  - description: it is also used as a delta for incrementing round_ (see below)
    to obtain the next round_ number. log(max_num_peers_) number of lower bits
    in round_ store the id_ of the peer. since current round_ value already
    contains the id of the current leader, we do not use a separate leader
    variable. we initialize round_ to max_num_peers_, which is the first invalid
    peer id, to indicate that at the beginning there is no leader.

- log_
  - synopsis: a log of instance_t's.
  - init: empty
  - description: abstractly, it is a circular buffer with a head and tail, where
    instances are added at the head and removed at the tail. although it may
    appear that log_ can be easily implemented as a fixed-size circular buffer,
    it has other requirements that makes circular buffer a bad choice. for
    example, a circular buffer has a fixed size: once it is full, no more
    insertions are possible. in our scenario, we only remove an instance from
    the tail_ (see below) when all peers have told us their tail_ value. so, if
    only one of the peers becomes partitioned, it will prevent us moving tail_,
    and we will fill the circular buffer and stall the progress because one of
    the peers is temporarily partitioned. therefore, we are going to use a
    hashmap from index to instance for this data structure. it is possible to
    implement a resizeable circular buffer (e.g. boost library provides one),
    but for now we are keeping things simple and use a hashmap. our intuition is
    that hashmap should behave like a circular buffer performance-wise, but a
    more thorough evaluation of this is a future work.
  - reading threads:
    - TBD
  - writing threads:
    - TBD

- head_
  - synopsis: index of the last command in the log_.
  - init: 0
  - description: used for determining the index of a new command. a new command
    is assigned the index of ++head_.
  - reading threads:
    - TBD
  - writing threads:
    - TBD

- tail_
  - synopsis: index of the last executed command in the log_.
  - init: 0
  - description: once a command is committed, it is safe to execute it on the
    state machine if all the preceding commands are committed and executed.
    hence, every time a command is committed, if its index is equal to tail_ +
    1, then that command is executed and tail_ is incremented.
  - reading threads:
    - TBD
  - writing threads:
    - TBD

- leader_tail_
  - synopsis: index of the last executed command in the log_ of the leader.
  - init: 0
  - description: the leader will inform us of its tail via heartbeats and we
    will try advance our tail_ by executing commands until leader_tail_.
  - reading threads:
    - TBD
  - writing threads:
    - TBD

- min_tail_
  - synopsis: minimum of tails of all peers.
  - init: 0
  - description: helps with pruning logs. all the commands with indices <=
    min_tail_ can be safely pruned from all peers' logs since these commands have
    been applied to the state machine of every peer. this variable is updated
    through heartbeats. as a response to a heartbeat, the leader receives tail_
    from all peers, computes the min_tail_, and then in the next heartbeat sends
    it to the followers. the leader should send an updated min_tail_ only if it
    has received tails from all the peers. Otherwise, liveness will be violated
    as described in SCENARIO-1 (see at the bottom).
  - reading threads:
    - TBD
  - writing threads:
    - TBD

- leader_min_tail_
  - synopsis: index of the last globally executed command known to the leader.
  - init: 0
  - description: the leader will inform us of global min_tail_ via heartbeats
    and we will trim our log_ by deleting instances from min_tail_ until
    leader_min_tail_.
  - reading threads:
    - TBD
  - writing threads:
    - TBD

- id_
  - synopsis: an integer representing the id of the peer
  - init: passed in as a command-line argument
  - note:
    - assert(id_ < max_num_peers_)
  - reading threads: N/A
  - writing threads: N/A

- ready_
  - synopsis: a boolean indicating if the leader is ready to accept commands:
  - init: false
  - description: true if this peer is leader and it is ready to accept commands.
    when a new leader elected, it receives a log that is merged from the
    responses of all other peers. to recover the latest states of the instances
    in this newly received log, the newly elected leader needs to run phase2 on
    each one of them. this may take a while, and to keep things simple, we just
    tell clients to retry during that period; once the leader has fully
    recovered the log, it sets ready_ to true and now clients are served.
  - reading threads:
    - TBD
  - writing threads:
    - TBD

- last_heartbeat_
  - synopsis: time of the last heartbeat from current leader.
  - init: 0
  - reading threads:
    - TBD
  - writing threads:
    - TBD

- heartbeat_interval_
  - synopsis: time between heartbeats.
  - init: 300ms
  - reading threads: N/A
  - writing threads: N/A

- round_
  - synopsis: an integer representing the current paxos round.
  - init: max_num_peers_
  - description: also known as ballot or term, this number represents the
    current round known to peer, during which this or some other peer is the
    leader. it is initialized to id_, which forms its lower bits and incremented
    when a peer needs to send prepare messages (i.e. start an election). to
    obtain its next value, round_ is incremented by max_num_peers_, which is set
    to 2^n where n is the number of bits used for storing the id_ at the lower
    bits. for now, we initialize max_num_peers_ to 16, which is 2^4; that is, we
    use the lower 4 bits for storing the id_, which means id_ can be any number
    in [0,16) and the size of peers_ must be <= max_num_peers_.
  - reading threads:
    - TBD
  - writing threads:
    - TBD

== PAXOS OBJECT METHODS ========================================================

- next_round#() -> int
  round_ += max_num_peers_ # increment the round portion by 1
  round_ &= id_            # set the id portion to this peer's id
  return round_

- leader() -> int
  return round_ & (max_num_peers_ - 1)

- i_am_leader() -> bool
  return leader() == id_

- someone_else_is_leader() -> bool
  return round_ != max_num_peers_

- agree_and_execute(command: command_t) -> accept_result_t
  if i_am_leader():
    if ready_:
      return accept(command, ++head_)
    return accept_result_t{type_: retry, result_: N/A, leader_: N/A}
  # someone else is leader
  if someone_else_is_leader():
    return accept_result_t{type_: leader, leader_: leader()}
  # no one is leader---an election is in progress
  return accept_result_t{type_: retry, result_: N/A, leader_: N/A}

- accept(command: command_t, index: int) -> accept_result_t
  num_responses = 0
  ok_responses = vector
  cv, mu
  request = accept_rpc_request{command: command, index: index, round: round_}
  for each peer p {
    run closure in a separate thread {
      response = p.acceptRPC(request)
      lock(mu)
      ++num_responses
      if response.type_ == ok:
        ok_responses.push(response)
      else if response.type_ == reject:
        round_ = response.round_
      # else it is a timeout error; we do nothing
      unlock(mu)
      cv.notify_one()
    }
  }
  lock(mu)
  while i_am_leader() &&
        ok_responses.size() <= peers_.size()/2 &&
        num_responses != peers_.size():
    cv.wait(mu)
  # one of the above three conditions is false; handle each, starting with the
  # most likely one
  if ok_responses.size() > peers_.size()/2: # we have quorum
    # our accept handler must have inserted the command
    assert(log_[index] != null log_[index].status == in-progress)
    log_[index].status = committed
    # execute as much as we can.
    lock(tail_mu_)
    assert(index > tail_)
    while (index != tail_+1)
      tail_cv_.wait(tail_mu_)
    result = execute(command)
    ++tail_
    unlock(tail_mu_)
    tail_cv_.notify_one()
    return accept_result_t{type_: result_.type_, value_: result_.value_, leader_: N/A}
  if someone_else_is_leader():
    return accept_result_t{type_: leader, result_: N/A, leader_: leader()}
  # multiple timeout responses
  return accept_result_t{type_: retry, result_: N/A, leader_: N/A}

- accept_handler(message: accept_rpc_request):
  # common case
  i = instance_t{round_: round_, command_: message.command, index_:
                 message.index, state_: in-progress}
  if message.round_ == round_:
    log_[message.index_] = i
    return accept_rpc_response{type_: ok, round_: N/A}
  # stale message
  if message.round_ < round_:
    return accept_rpc_response{type: reject, round: round_}
  # someone else is a leader
  if instance.round > round_:
    round_ = message.round_
    log_[message.index_] = i
    return accept_rpc_responset{type_: ok, round_: N/A}

- prepare() -> prepare_result_t:
  num_responses = 0
  ok_responses = vector
  cv, mu
  request = prepare_rpc_request{tail_: tail_, round_: next_round#()}
  for each peer p {
    run closure in a separate thread {
      response = p.prepareRPC(request)
      lock(mu)
      ++num_responses
      if response.type_ == ok:
        ok_responses.push(response)
      else if response.type_ == reject:
        round_ = response.round_
      # else it is a timeout error; we do nothing
      unlock(mu)
      cv.notify_one()
    }
  }
  lock(mu)
  while i_am_leader() &&
        ok_responses.size() <= peers_.size()/2 &&
        num_responses != peers_.size()
    cv.wait(mu)
  # one of the above three conditions is false; handle each, starting with the
  # most likely one
  if ok_responses.size() > peers_.size()/2: # we have quorum
    log = merge(ok_responses)
    return prepare_result_t{type_: ok, log_: log}
  if someone_else_is_leader():
    return prepare_result_t{type_: reject, log_: N/A}
  # multiple timeout responses
  return prepare_result_t{type_: timeout, log_: N/A}

- prepare_handler(message: prepare_rpc_request):
  # common case for phase1
  if message.round >= round_:
    round_ = message.round_
    return prepare_rpc_response_t{type_: ok, round_: N/A, log_: log_[tail+1:]}
  # stale messages
  return prepare_rpc_response_t{type_: reject, round_: round_, log_: N/A}

- prepare_thread():
  for (;;) {
    sleep until we are not a leader
    for (;;) {
      sleep(heartbeat_interval_ + random(10, heartbeat_interval_))
      if time::now() - last_heartbeat_ < heartbeat_interval_:
        continue
      prepare_result_t result = prepare()
      while (result.type_ == timeout)
        result = prepare()
      if result.type_ == reject: # someone else is a leader
        continue
      # we are a leader
      wake up heartbeat_thread
      replay(result.log_)
      ready_ = true
      break
    }
  }

- heartbeat_thread():
  for (;;) {
    sleep until we are a leader
    for (;;) {
      num_responses = 0
      ok_responses = vector
      cv, mu
      request = heartbeat_rpc_request{min_tail_: min_tail_}
      for each peer p {
        run closure in a separate thread {
          response = p.heartbeatRPC(request)
          lock(mu)
          ++num_responses
          if response.type_ == ok
            ok_responses.push(response)
          else if response.type_ == reject
            round_ = response.round_
          # else it is a timeout error; we do nothing
          unlock(mu)
          cv.notify_one()
        }
      }
      lock(mu)
      while i_am_leader() &&
            ok_responses.size() <= peers_.size()/2 &&
            num_responses != peers_.size():
        cv.wait(mu)
      if someone_else_is_leader():
        break
      sleep(heartbeat_interval_)
    }
  }

- heartbeat_handler(message: heartbeat_rpc_request):
  # common case for the heartbeat
  if message.round >= round_:
    round_ = response.round_
    leader_tail_ = message.tail_
    leader_min_tail_ = message.min_tail_
    return heartbeat_rpc_response{type_: ok, round_: N/A, tail_: tail_}
  # stale messages
  if message.round < round_:
    return heartbeat_rpc_response{type_: reject, round_: round_, tail_: N/A}

- execute_and_trim_thread():
  for (;;) {
    sleep until we are not a leader
    for (;;) {
      while min_tail_ < leader_min_tail_:
        ++min_tail_
        del log_[min_tail_];
      while tail_ < leader_tail_:
        ++tail_
        if log_[tail_].command_.type != get:
          execute(log_[tail_].command_)
      if someone_else_is_leader():
        break
      sleep(heartbeat_interval_);
    }
  }
== SCENARIOS ===================================================================

SCENARIO-1: Consider peers A, B, C, D, E, where A is the leader, and all peers
have synchronized logs with last_executed = 10. The peer E experiences a
partition, and in the meantime, A, B, C, D commit a lot more commands and each
increase their last_executed to 15, 14, 13, 14, respectively, whereas E's
last_executed remains at 10. A, the leader, receives last_executed from just B,
C, and D, and sends 13 as the min_last_executed back to peers, and A, B, C, and
D prune the commands up to and including 13. In the meantime, a client contacts
E, which at this point thinks (due to partition) that A is dead. So E increments
proposal# and starts phase1, times out, and keeps retrying phase1, with higher
and higher proposal#. At some point, the partition is fixed, and E sends
proposal# higher than A's current proposal# and asks for the log entries
starting at 11; but none of the peers has these entries, and the cluster is
stuck: the peers do not have the log entries so they cannot reply, and E will
just keep retrying phase1 with higher proposal#.

== TODO ========================================================================

- How to handle gaps?

  Currently, if a peer temporarily disconnects and then reconnects, then it will
  have a gap in its log. it will not be able to execute entries past the gap, it
  will not be able to prune its log, which will prevent everyone else from
  pruning their logs. when we have a gap like this, we should recover it by
  asking other peers. or we should resort to using log pruning that persists the
  state machine to disk and prunes the log without hearing from the peers. we do
  not implement this at the moment: if a peer temporarily disconnects and
  accrues a gap, then log pruning will be stuck on all processes.

- how to let peers know the committed? we can do it with the heartbeat, but
  should we, given that we already let everyone know executed entries?

  - the difference between min_tail_ and the committed entries is that we can
    only communicate min_tail_ if we have received the tails of all peers,
    whereas we can communicate the committed entries once we have the responses
    from the majority.

- handle duplicate responses due to retries

  - we will handle this by having gRPC retry RPC calls.

- imagine a scenario that there is a gap in the log, like [a, b, _, d] and once
  the thread1 commits d, it starts to wait until command at index 2 is executed
  and thread1 is woken up. at that moment, this machine stops being a leader,
  and someone else starts to run. they receive the log state, and eventually,
  they determine what goes into 2, and eventually, they notify this peer about
  the state of the log. then, we should wake up thread1)

- evaluate the choice of a resizeable circular buffer (see how boost implements
  it) for log on the performance.

- evaluate the choice of not sending messages to self on performance.

== SCRATCH SPACE ===============================================================

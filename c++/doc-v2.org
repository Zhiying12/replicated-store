* TYPES

- struct command_t
  - synopsis: a key-value command
  - fields:
    - type_: enum {get, put, del}
    - key_: string
    - value_: string (valid only if type_ == put)

- enum state_t: {in-progress, committed, executed}

- struct instance_t
  - synopsis: entry in the log.
  - fields:
    - round_: round of the instance
    - command_: command_t
    - index_: index of the command in the log
    - state_: state_t
    - client-id_: id of the client in the leader that issued the command
  - description: each instance starts in the in-progress state, and changes to
    the committed state once quorum has voted for it. we do not need a separate
    executed state because we are tracking it with the tail_ (see below). after
    the command is executed, we look for the client-id in the global map that
    maintains client-id to socket mapping. if found, then this is the leader
    that created the instance, and we write the response to the client;
    otherwise, this is a follower and we do nothing.

- struct accept_result_t
  - synopsis: return type of replicate() and accept()
  - fields:
    - type_: enum {ok, reject, retry}
    - leader_: int (if type_ == reject then contains id of the current leader_)

- struct prepare_result_t
  - synopsis: return type of prepare()
  - fields:
    - type_: enum {ok, reject, timeout}
    - logs_: vector<instance_t[]>

* PROTOBUFS

- message accept_rpc_request
  - round_: round of the sender
  - command_: command_t
  - index_: int; index for the command_ in log_
  - status_: enum {in-progress, committed, executed}
  - client-id_: int; id of the client initiating command_

- message accept_rpc_response
  - type_: enum {ok, reject, timeout}
  - round_: round of the remote peer (valid only if type_ == reject)

- message prepare_rpc_request
  - round_: round of the sender

- message prepare_rpc_response
  - type_: enum {ok, reject, timeout}
  - round_: round of the remote peer (valid only if type_ == reject)
  - log_: instances since min_tail_ in the corresponding prepareRPC request (valid
    only if type_ == ok}

- message heartbeat_rpc_request
  - round_: local round
  - tail_: int (tail of the leader)
  - min_tail_: int (minimum of all tail_s seen so far)

- message heartbeat_rpc_response
  - tail_: int (local tail_)

* OBJECTS

- Replicant

  - members
    - kv_: KVStore
    - c_: Consensus
    - log_: Log
    - tp_: Threadpool
    - l_: socket
    - exe_cv_: condition_variable
    - exe_mu_: mutex
    - map_: map from client_id(int) to socket object
    - client_id_: id of the next client that will connect
    - id_: id of the peers
    - num_peers_: number of peers

  - methods

    - constructor(cfg: config)
      kv_ = new(KVStore)
      c_ = new(Consensus(cfg))
      log_ = new(Log(&exe_cv_, &exe_mu_))
      id_ = config["id"]
      client_id_ = id_
      num_peers_ = confg["num_peers"]

    - next_client_id(void) -> int
      id = client_id_
      client_id_ += num_peers_
      return id

    - run(void)
      thread(executor).start()
      for (;;) {
        socket = accept(l)
        client-id = next_client_id()
        map_[client-id] = socket
        thread(handle_client, client-id).start()
      }

    - handle_client(client-id: int)
      it = map_[client-id]
      for (;;) {
        cmd = read_command(it->socket)
        if (cmd)
          tp_.post(replicate, cmd, client-id)
        else
          break
      }
      close(it->socket)
      del it

    - replicate(cmd: command, client-id: int)
      r = c.replicate(cmd, client-id)
      if r.type_ != ok
        it = map_[client-id]
        send(it->socket, r)
        close(it->socket)
        del it

    - executor(void)
      for (;;) {
        exe_mu_.lock()
        while (!log_.executable())
          exe_cv_.sleep()
        exe_mu_.unlock()
        do {
          client-id, result = log_.execute(kv)
          respond(client-id, result)
        } while (log_.executable())
      }

    - respond(client-id, result)
      # responds to the client with the result of the command execution. this
      # function will respond to the client only if the client originally sent
      # the request to this peer when it was a leader. this constraint is
      # implicitly enforced by having each peer assign a unique id to each
      # client.
      it = map_[client-id]
      if it != null
        send(it->socket, result)

- Log

  - members

    - head_: index of the highest-numbered instance in the log. the log starts
      at index 1; therefore, head_ is initialized to 0, signifying an empty log.

    - tail_: index of the last executed instance. initialized to 0.

    - min_tail_: minimum tail_ of all peers known to peer. initialized to 0.

    - min_tail_leader_: latest minimum tail_ at the leader. initialized to 0.

    - log_: a map from int to an instance_t

    - cv_, mu_: pointers to a condition variable and mutex to wake up the
      executor thread.

  - methods

    - constructor(cv: *condition_variable, mu: *mutex)
      cv_ = cv
      mu_ = mu
      head_ = tail_ = min_tail_ = 0

    - tail(void) -> int
      return tail_

    - min_tail(void) -> int
      return min_tail_

    - advance_head(void) -> int
      return ++head_

    - executable(void) -> bool
      # returns true if the log contains an executable instance, i.e. the
      # instance right after tail_ is committed.
      return log_[tail_+1] != empty && log_[tail_+1].status == committed

    - execute(kv: KVStore) -> (client-id, result)
      # executes the next executable instance in the log, updates the instance's
      # status, increments tail_, and returns the result and the id of the
      # client that originated the command.
      assert(executable())
      instance = &log_[tail_+1]
      result = kv.execute(instance.cmd)
      ++tail_
      return (instance.client-id_, result)

    - commit(index_: int)
      # sets the status of the instance at index to committed and wakes up the
      # executor thread if the log is executable.
      exe_mu_.lock()
      # it must never be the case that the log has no instance at index_ because
      # if we are committing index_, we must have sent out an accept for it and
      # received a positive response from quorum; given that we are a leader, we
      # must have sent an accept to ourselves via loopback and the accept
      # handler must have installed the instance at index_. technically, there
      # is a remote possibility of us receiving a response from remote peers
      # before a local accept handler runs, but it is more likely that this
      # assertion will trigger due to another problem than that.
      assert(log_[index_] != empty)

      # when running prepare, we will ask peers, including ourselves, to send us
      # their log starting at their min_tail_ and merge it to our log. then we
      # will run accept on all instances starting at min_tail_. hence, we may
      # run accept on an instance that is already committed or even executed in
      # our log. our accept handler will not touch log_ for such instances but
      # it will respond with an accept and eventually, we may run commit for
      # such instances, in which case we will end up here. for those instances,
      # commit will not do anything. hence, we will only update an instances
      # status to committed only if it is in in-progress state.
      if log_[index_].status == in-progress
        log_[index_].status = committed
      else
        return
      if (executable())
        exe_cv_.notify_one()
      exe_mu_.unlock()

    - commit_until(tail: int, round_: int)
      # sets the status of all the instances from tail_ until tail and wakes up
      # the executor thread.
      exe_mu_.lock()
      for (int i = tail_+1; i <= tail; ++i)
        # it is possible that we receive heartbeat before we receive the accept
        # message; therefore, we should handle the case where we have a gap in
        # the log; in that case, we will break out of the loop and try
        # committing the next time we receive heartbeat from the leader;
        # hopefully, by that time, we will receive the accept message and the
        # gap will disappear.
        if (log_[index] == empty)
          break
        if (log_[index].round_ == round_)
          log_[i].status = committed
      if (executable())
        exe_cv_.notify_one()
      exe_mu_.unlock()

    - trim_until(min_tail_leader_: int)
      while min_tail_ < min_tail_leader_
        ++min_tail_
        assert(log_[min_tail_].status == executed)
        del log_[min_tail_]

    - append(instance: instance_t)
      i = instance.index_
      if i <= min_tail_
        return
      if log_[i] == empty
        log_[i] = instance
        log_[i].status = in-progress
      else if log_[i].status == committed or executed
        assert(instance.status_ != committed or executed || log_[i].cmd_ == instance.cmd_)
      else if log_[i].round_ < instance.round_
        log_[i] = instance
        log_[i].status = in-progress

      head_ = max(head_, i)

    - merge(logs: vector<instance_t[]>)
      for log in logs
        for instance in log
          if instance.index_ > min_tail_
            append(instance)

    - new_instances() -> instance_t[]
      # return instances since min_tail_

- Consensus

  - members
    - peers
    - head_
    - log_: Log
    - id_
    - ready_
    - last_heartbeat_
    - heartbeat_interval_
    - round_

  - methods

    - constructor(cfg: config)

    - replicate(cmd: command, client-id: int) -> accept_result_t
      if i_am_leader()
        if ready_
          return accept(cmd, log_.advance_head(), client-id)
        return accept_result_t{type_: retry, leader_: N/A}
      if someone_else_is_leader()
        return accept_result_t{type_: reject, leader_: leader()}
      # election in progress
      return accept_result_t{type_: retry, leader_: N/A}

    - accept(cmd: command, index: int, status: status_t, client-id: int) -> accept_result_t
      num_responses = 0
      num_ok_responses = 0
      cv, mu
      request = accept_rpc_request{command_: cmd,
                                   index_: index,
                                   round_: round_,
                                   status_: status,
                                   client-id_: client-id}
      for each peer p {
        run closure in a separate thread {
          response = p.acceptRPC(request)
          lock(mu)
          ++num_responses
          if response.type_ == ok:
            ++ok_responses
          else if response.type_ == reject:
            round_ = response.round_
          # else it is a timeout error; we do nothing
          unlock(mu)
          cv.notify_one()
        }
      }
      lock(mu)
      while i_am_leader() &&
            num_ok_responses <= peers_.size()/2 &&
            num_responses != peers_.size():
        cv.wait(mu)

      if num_ok_responses > peers_.size() / 2
        log_.commit(index)
        return accept_result_t{type_: ok, leader_: N/A}
      if someone_else_is_leader()
        return accept_result_t{type_: reject, leader_: leader()}
      # RPCs timed out
      return accept_result_t{type_: retry, leader_: N/A}

    - accept_handler(message: accept_rpc_request)
      if message.round_ >= round_:
        round_ = message.round_
        instance = instance_t{round_: message.round_,
                              command_: message.command_,
                              index_: message.index_,
                              state_: in-progress
                              client-id_: message.client-id_}
        log_.append(instance)
        return accept_rpc_response{type_: ok, round_: N/A}
      # stale message
      return accept_rpc_response{type: reject, round: round_}

    - prepare() -> prepare_result_t:
      num_responses = 0
      ok_logs = vector<instance_t[]>
      cv, mu
      request = prepare_rpc_request{round_: next_round#()}
      for each peer p {
        run closure in a separate thread {
          response = p.prepareRPC(request)
          lock(mu)
          ++num_responses
          if response.type_ == ok:
            ok_logs.push(response.log_)
          else if response.type_ == reject:
            round_ = response.round_
          # else it is a timeout error; we do nothing
          unlock(mu)
          cv.notify_one()
        }
      }
      lock(mu)
      while i_am_leader() &&
            num_ok_responses <= peers_.size()/2 &&
            num_responses != peers_.size()
        cv.wait(mu)
      # one of the above three conditions is false; handle each, starting with the
      # most likely one
      if num_ok_responses > peers_.size()/2: # we have quorum
        return prepare_result_t{type_: ok, log_: ok_logs}
      if someone_else_is_leader():
        return prepare_result_t{type_: reject}
      # multiple timeout responses
      return prepare_result_t{type_: timeout}

    - prepare_handler(message: prepare_rpc_request):
      # common case for phase1
      if message.round >= round_:
        round_ = message.round_
        return prepare_rpc_response_t{type_: ok,
                                      round_: N/A,
                                      log_: log_.new_instances()}
      # stale messages
      return prepare_rpc_response_t{type_: reject, round_: round_, log_: N/A}

    - prepare_thread():
      for (;;) {
        sleep until follower
        for (;;) {
          sleep(heartbeat_interval_ + random(10, heartbeat_interval_))
          if time::now() - last_heartbeat_ < heartbeat_interval_:
            continue
          ready_ = true
          prepare_result_t result = prepare()
          if result.type_ != ok:
            continue
          # we are a leader
          wake up heartbeat_thread
          log_.merge(result.logs_)
          replay()
          ready_ = true
          break
        }
      }

    - replay():
      index_ = log_.get_tail_index() + 1

      while (index_ <= head_) {
        instance = log_.get_instance(index)
        assert(instance != NULL)

        # The instance needs to accept again
        if instance.state_ == in-progress:
          accept_result_ = accept(instance.command, index)
          if accept_result_.type == leader:
            break
          if accept_result_.type == retry:
            continue

        # Else, the instance is commited, nothing needs to be done

        ++index
      }

    - heartbeat_thread():
      for (;;) {
        sleep until leader
        num_responses = 0
        ok_responses = vector
        cv, mu
        min_tail = log_.min_tail()
        for (;;) {
          request = heartbeat_rpc_request{round_: round_,
                                          tail_: log_.tail()
                                          min_tail_: min_tail}
          for each peer p {
            run closure in a separate thread {
              response = p.heartbeatRPC(request)
              lock(mu)
              ++num_responses
              if response.ok:
                ok_responses.push(response)
              unlock(mu)
              cv.notify_one()
            }
          }
          lock(mu)
          while i_am_leader() && num_responses != peers_.size():
            cv.wait(mu)
          if ok_responses.size() == peers_.size():
            min_tail = min(ok_responses)
          if someone_else_is_leader():
            break
          sleep(heartbeat_interval_)
        }
      }

    - heartbeat_handler(message: heartbeat_rpc_request):
      if message.round >= round_:
        last_heartbeat_ = time::now()
        round_ = message.round_
        log_.commit_until(message_.tail_, round_)
        log_.trim_until(message_.min_tail_)
      # stale message
      return heartbeat_rpc_response{tail_: log_.tail()}

== TODO ========================================================================

- we can handle gaps if there is a leader election, but if there is no leader
  election, a follower that temporarily experienced a network partition will
  hinder global progress. we need to come up with an alternative recovery
  mechanism to handle this problem.

- How to handle gaps?

  Currently, if a peer temporarily disconnects and then reconnects, then it will
  have a gap in its log. it will not be able to execute entries past the gap, it
  will not be able to prune its log, which will prevent everyone else from
  pruning their logs. when we have a gap like this, we should recover it by
  asking other peers. or we should resort to using log pruning that persists the
  state machine to disk and prunes the log without hearing from the peers. we do
  not implement this at the moment: if a peer temporarily disconnects and
  accrues a gap, then log pruning will be stuck on all processes.

- how to let peers know the committed? we can do it with the heartbeat, but
  should we, given that we already let everyone know executed entries?

  - the difference between min_tail_ and the committed entries is that we can
    only communicate min_tail_ if we have received the tails of all peers,
    whereas we can communicate the committed entries once we have the responses
    from the majority.

- handle duplicate responses due to retries

  - we will handle this by having gRPC retry RPC calls.

- imagine a scenario that there is a gap in the log, like [a, b, _, d] and once
  the thread1 commits d, it starts to wait until command at index 2 is executed
  and thread1 is woken up. at that moment, this machine stops being a leader,
  and someone else starts to run. they receive the log state, and eventually,
  they determine what goes into 2, and eventually, they notify this peer about
  the state of the log. then, we should wake up thread1)

- evaluate the choice of a resizeable circular buffer (see how boost implements
  it) for log on the performance.

- evaluate the choice of not sending messages to self on performance.

- evaluate the choice of lazy (via piggybacking onto heartbeats) vs eager (via
  piggybacking onto accepts) sending commit messages to followers.

== SCRATCH SPACE ===============================================================
    0    1   2   3   4
x: |a,x|b,x|c,x|d,x|   |             min_tail = 1, tail = 3

f1:|a,x|b,x|c,x|d,x|   |             min_tail = 1, tail = 3

f2:|a,x|b,x|c  |d,i|   |             min_tail = 1, tail = 1


min_tail = 1

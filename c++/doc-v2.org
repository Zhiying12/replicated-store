* TYPES

- struct Command
  - synopsis: a key-value command
  - fields:
    - type_: enum { get, put, del }
    - key_: string
    - value_: string (valid only if type_ == put)

- struct Result
  - synopsis: result of a key-value command
  - fields:
    - ok_: bool (true if result is a success, false otherwise)
    - value_: string (if ok_ == false, contains error message, otherwise
      contains return value for a get command or empty for put and del)

- struct Instance
  - synopsis: entry in the log. each instance starts in the in-progress state,
    and changes to the committed state once quorum has voted for it, and changes
    to executed state once the executor thread has its command executed. once
    the command is executed, we look for the client-id in the global map that
    maintains client-id to socket mapping. if found, then this is the leader
    that created the instance, and we write the response to the client;
    otherwise, this is a follower and we do nothing.
  - fields:
    - ballot_: ballot of the instance
    - index_: index of the command in the log
    - client-id_: id of the client in the leader that issued the command
    - state_: enum { in-progress, committed, executed }
    - command_: Command

- struct accept_result_t
  - synopsis: return type of replicate() and accept()
  - fields:
    - type_: enum {ok, reject, retry}
    - leader_: int (if type_ == reject then contains id of the current leader_)

- struct prepare_result_t
  - synopsis: return type of prepare()
  - fields:
    - type_: enum {ok, reject, timeout}
    - logs_: vector<instance_t[]>

* PROTOBUFS

- message accept_rpc_request
  - ballot_: ballot of the sender
  - command_: command_t
  - index_: int; index for the command_ in log_
  - client-id_: int; id of the client initiating command_

- message accept_rpc_response
  - type_: enum {ok, reject, timeout}
  - ballot_: ballot of the remote peer (valid only if type_ == reject)

- message prepare_rpc_request
  - ballot_: ballot of the sender

- message prepare_rpc_response
  - type_: enum {ok, reject, timeout}
  - ballot_: ballot of the remote peer (valid only if type_ == reject)
  - log_: instances since global_last_executed_ in the corresponding prepareRPC
    request (valid only if type_ == ok}

- message heartbeat_rpc_request
  - ballot_: local ballot
  - last_executed_: int
  - global_last_executed_: int

- message heartbeat_rpc_response
  - last_executed_: int (local last_executed_)

* OBJECTS

- Replicant

  - members
    - kv_: KVStore
    - c_: Consensus
    - log_: Log
    - tp_: Threadpool
    - l_: socket
    - map_: map from client_id(int) to socket object
    - client_id_: id of the next client that will connect
    - id_: id of the peers
    - num_peers_: number of peers

  - methods

    - constructor(cfg: config)
      kv_ = new(KVStore)
      c_ = new(Consensus(cfg))
      log_ = new(Log())
      id_ = config["id"]
      client_id_ = id_
      num_peers_ = confg["num_peers"]

    - next_client_id(void) -> int
      id = client_id_
      client_id_ += num_peers_
      return id

    - run(void)
      thread(executor).start()
      for (;;) {
        socket = accept(l)
        client-id = next_client_id()
        map_[client-id] = socket
        thread(handle_client, client-id).start()
      }

    - handle_client(client-id: int)
      it = map_[client-id]
      for (;;) {
        cmd = read_command(it->socket)
        if (cmd)
          tp_.post(replicate, cmd, client-id)
        else
          break
      }
      close(it->socket)
      del it

    - replicate(cmd: command, client-id: int)
      r = c.replicate(cmd, client-id)
      if r.type_ != ok
        it = map_[client-id]
        send(it->socket, r)
        close(it->socket)
        del it

    - executor(void)
      for (;;) {
        client-id, result = log_.execute(kv)
        respond(client-id, result)
      }

    - respond(client-id, result)
      # responds to the client with the result of the command execution. this
      # function will respond to the client only if the client originally sent
      # the request to this peer when it was a leader. this constraint is
      # implicitly enforced by having each peer assign a unique id to each
      # client.
      it = map_[client-id]
      if it != null
        send(it->socket, result)

- Log
  # We can think of Log as an unbounded producer-consumer queue. From this
  # perspective, the execute method acts as the consume method of a queue, and
  # the commit method acts as a produce method of a queue. Technically,
  # instances are inserted into the queue via the append method; however, they
  # do not become executable until they are committed by calling commit on the
  # instance. The Log is unbounded because the instances will usually be
  # executed quickly; therefore, wake up happens only one way: the thread that
  # commits wakes up the executor thread to execute instances.

  - members

    - log_: a map from int to an instance_t

    - last_index_: index of the highest-numbered instance in the log. the log
      starts at index 1; therefore, last_index_ is initialized to 0, signifying
      an empty log.

    - last_executed_: index of the last executed instance. initialized to 0.

    - global_last_executed_: index of the last executed instance on all peers
      known to this peer. initialized to 0.

    - mu_: the mutex of the object that needs to be acquired before the object
      is modified.

    - cv_: the condition variable on which the execute method sleeps and commit
      method signals.

  - public methods

    - constructor()
      last_index_ = 0
      last_executed_ = 0
      global_last_executed_ = 0

    - last_executed(void) -> int
      acquire mu_ and release on exit
      return last_executed_

    - global_last_executed(void) -> int
      acquire mu_ and release on exit
      return global_last_executed_

    - advance_last_index(void) -> int
      acquire mu_ and release on exit
      return ++last_index_

    - execute(kv: KVStore) -> (client-id, result)
      # As described above, this method acts as a consume method of a
      # producer/consumer queue. Therefore, it sleeps until it is woken up by
      # someone calling commit (i.e. produce) method of the queue. Once woken
      # up, it executes one instance, sets the state of the instance to
      # executed, increments last_executed_, and returns the result and the id
      # of the client that originated the command.
      mu_.lock()
      while not is_executable():
        cv_.wait()

      instance = &log_[last_executed_+1]
      result = kv.execute(instance.cmd)
      ++last_executed_
      return (instance.client-id_, result)

    - commit(index_: int)
      # sets the state of the instance at index to committed and possibly wakes
      # up the executor thread if the log is executable.

      * # commit is called exclusively by the leader, after it sends out accept
        # to all peers, including itself, and receiving a postivie response from
        # the majority of peers. the accept handler in every peer inserts an
        # entry into the log and then respond with an ok if successful. most of
        # the time, the leader will immediately receive an ok from its own rpc
        # handler before receiving ok from the remote peers; therefore, by the
        # time it calls commit, it is almost guaranteed to have an entry at the
        # index_, and calling commit is fine. in the rare cases when the leader
        # receives ok responses from remote peers before its own rpc handler has
        # run and inserted an entry and responded, we busy wait to avoid crash.
      for (;;)
        mu_.lock()
        if log_[index_] == empty
          break
        mu_.unlock()

      * # when running prepare, we will ask peers, including ourselves, to send us
        # their log starting at their global_last_executed_ and merge it to our
        # log. then we will run accept on all instances starting after
        # global_last_executed_. hence, we may run accept on an instance that is
        # already committed or even executed in our log. our accept handler will
        # not touch log_ for such instances but it will respond with an accept
        # and eventually, we may run commit for such instances, in which case we
        # will end up here. for those instances, commit must be a no-op. hence,
        # we will only update an instances state to committed only if it is in
        # in-progress state.
      if log_[index_].state == in-progress
        log_[index_].state = committed

      # we must do this check every time because it may be an entry that we
      # merged into our log from a remote peer that was already in committed
      # state. in this case, we should wake up the thread to execute the entry
      # on our state machine.
      if (executable())
        cv_.notify_one()
      mu_.unlock()

    - commit_until(last_executed: int, ballot_: int)
      # sets the state of all the instances from last_executed_ until tail and
      # wakes up the executor thread if necessary.
      mu_.lock()
      for (int i = last_executed_+1; i <= last_executed; ++i)
        * # we may receive a heartbeat before we receive the accept message;
          # therefore, the heartbeat handler will run this function while there is
          # a gap in the log. when we see a gap, we break out of the loop and try
          # committing the next time we receive heartbeat from the leader;
          # hopefully, by that time, we will have received the accept message and
          # the gap will disappear.
        if (log_[index] == empty)
          break
        * # as a follower, we will usually have in-progress instances in our log;
          # in the common case, we will receive a higher tail value from the
          # leader and we will catch up by committing instances in our own log.
          # however, it is possible that (1) we experience a partition, (2) a
          # new leader emerges and establishes new commands for those instances,
          # and (3) we reconnect. now, if we receive a heartbeat with a higher
          # tail value then we shouldn't blindly commit instances in our log; we
          # should commit them only if the ballot numbers match (which
          # corresponds to the common case). otherwise, as a follower we will
          # just get stuck here and prevent global_last_executed_ from
          # advancing, until a new leader is elected and replays every instances
          # since global_last_executed_ and we discover the new commands and
          # update stale instances in our log.
        if (log_[index].ballot_ == ballot_)
          log_[i].state = committed
      if (executable())
        cv_.notify_one()
      mu_.unlock()

    - trim_until(global_last_executed: int)
      while global_last_executed_ < global_last_executed
        ++global_last_executed_
        assert(log_[global_last_executed_].state == executed)
        del log_[global_last_executed_]

    - append(instance_: instance_t)
      * # log invariants
        #
        # given that (1) the instances in the log must be executed in order, (2)
        # last_executed_ is the index of the last executed instance, and (3)
        # global_last_executed_ is the index of the last instance that was
        # executed in all peers, our log has the following invariants:
        #
        # (i1) there is no gap before or at last_executed_
        # (i2) there is no executed instance after last_executed_.
        # (i3) global_last_executed_ <= last_executed_
        # (i4) there are no instances at indices < global_last_executed_

      * # append() call-paths
        #
        # we call append() in two call-paths:
        #
        # (c1) when we are a follower and we receive an accept message, we call
        #      append() in accept_handler()
        # (c2) when we are a leader candidate and we send out prepare request
        #      and receive logs from the quorum, we call append() in
        #      log_.merge() to merge the received logs.

      * # case (1): instance_.index is pruned from log
        #
        # append() must be a no-op if we call it with an instance at an index
        # pruned from our log. it is possible to receive such an instance in
        # (c1), for example, if
        #
        # (1) we currently have global_last_executed_ = 13
        # (2) a new leader sends us a prepare request
        # (3) we respond by sending instances after global_last_executed_, e.g.
        #     (14, 15, 16)
        # (4) we receive a heartbeat with global_last_executed_ = 15 from the
        #     old leader
        # (5) we trim our log and set global_last_executed_ to 15
        # (6) we receive an accept from the new leader for the instance 14
        #
        # it is also possible to receive such an instance in case (c2), for
        # example, if
        #
        # (1) we currently have global_last_executed_ = 13
        # (2) we become a leader candidate and send prepare request to peers
        # (3) we receive a heartbeat with global_last_executed_ = 15 from the
        #     old leader
        # (4) we trim our log and set global_last_executed_ to 15
        # (5) we receive logs from the other peers who still have
        #     global_last_executed_ = 13
        #
        # we should ignore such instances.

      mu_.lock()
      i = instance_.index_
      if i <= global_last_executed_
        mu_.unlock()
        return

      * # before we jump to the next case, now that we know the instance_ is not
        # stale, we need to see if it is in executed state and change it to
        # committed to preserve (i2). that's because append may be called with
        # an instance in executed state in (c2): if we are a peer that got
        # partitioned and joined back, and we try to become a leader, we will
        # receive logs from other peers that have executed instances in their
        # logs. we need to reset the state of such instances back to committed
        # to ensure that such instances will be executed on our state machine.
        # we don't need to do anything to an instance that is in in-progress or
        # committed states -- if it is committed, then it will eventually get
        # executed, or if it is in-progress, it will either be committed or
        # updated. also, as an aside, in (c1), append will only be called with
        # instances in in-progress state because the instances are initialized
        # as such.
      if instance.state == executed
        instance.state = committed

      * # case (2): log_[instance_.index_] is empty
        #
        # in that case, it must be the case that i > last_executed_ due to (i1).
        #
        # (1) we assert i1.
        # (2) we insert instance_ to our log.
        # (3) we update the last_index_ and return
      if log_[i] == empty
        assert(i > last_executed_)
        log_[i] = instance_
        last_index_ = max(last_index_, i)
        mu_.unlock()
        return

      * # case (3): log_[instance_.index] is committed/executed
        #
        # append() must be a no-op if we call it with an instance at an index
        # that is already a committed or executed in our log; furthermore, in a
        # situation like this, instance_'s command must match the command in our
        # log, *independent of what instance_'s state is*. if instance_'s
        # state is in-progress, i.e. append() is being called in (c1), then it
        # must have learned the command from the quorum. if instance_'s state
        # is committed or executed, i.e. append() is being called in (c2), then
        # logs from other peers must contain the same command.
      if log_[i].state == (committed or executed)
        assert(log_[i].command_ == instance_.command_)
        mu_.unlock()
        return

      * # case (4): log_[instance_.index] is in-progress
        #
        # in this case, we should decide based on the value of ballot_.
        #
        # if log_[instance_.index].ballot_ < instance_.ballot_, we must update
        # our log because we may have a stale instance; such a scenario may
        # happen in (c2) when we receive a newer log from a peer.
        #
        # if log_[instance_.index].ballot_ == instance_.ballot_, it must be the
        # case that both instances have the same command. this may happen in
        # scenario (c1) when somehow we receive the same accept command twice.
        #
        # if log_[instance_.index].ballot_ > instance_.ballot_, we can ignore
        # the instance; such a scenario may happen in (c2) when we receive a
        # stale log from a peer.
        #
      if log_[i].ballot_ < instance_.ballot_
        log_[i] = instance
        mu_.unlock()
        return

      if log_[i].ballot_ == instance_.ballot_
        assert(log_[i].command_ == instance_.command_)
        mu_.unlock()
        return
      # ignore the case of log_[instance_.index_].ballot_ > instance_.ballot_

    - merge(logs: vector<instance_t[]>)
      for log in logs
        for instance in log
          append(instance)

    - new_instances() -> instance_t[]
      # return instances since global_last_executed_

  - private methods:

    - is_executable(void) -> bool
      # preconditions: mu_ must be held

      # returns true if the log contains an executable instance, i.e. the
      # instance right after last_executed_ is committed.
      return log_[last_executed_+1] != empty &&
        log_[last_executed_+1].state == committed

   - test methods:
     # these methods are used only in unit tests

     - at(index: int) -> pointer to instance
       return a constant pointer to the instance




- Consensus

  - members
    - peers
    - log_: Log
    - id_
    - ready_
    - last_heartbeat_
    - heartbeat_interval_
    - ballot_

  - methods

    - constructor(cfg: config)

    - replicate(cmd: command, client-id: int) -> accept_result_t
      if i_am_leader()
        if ready_
          return accept(cmd, log_.advance_last_index(), client-id)
        return accept_result_t{type_: retry, leader_: N/A}
      if someone_else_is_leader()
        return accept_result_t{type_: reject, leader_: leader()}
      # election in progress
      return accept_result_t{type_: retry, leader_: N/A}

    - accept(cmd: command, index: int, client-id: int) -> accept_result_t
      num_responses = 0
      num_ok_responses = 0
      cv, mu
      request = accept_rpc_request{command_: cmd,
                                   index_: index,
                                   ballot_: ballot_,
                                   client-id_: client-id}
      for each peer p {
        run closure in a separate thread {
          response = p.acceptRPC(request)
          lock(mu)
          ++num_responses
          if response.type_ == ok:
            ++ok_responses
          else if response.type_ == reject:
            ballot_ = response.ballot_
          # else it is a timeout error; we do nothing
          unlock(mu)
          cv.notify_one()
        }
      }
      lock(mu)
      while i_am_leader() &&
            num_ok_responses <= peers_.size()/2 &&
            num_responses != peers_.size():
        cv.wait(mu)

      if num_ok_responses > peers_.size() / 2
        log_.commit(index)
        return accept_result_t{type_: ok, leader_: N/A}
      if someone_else_is_leader()
        return accept_result_t{type_: reject, leader_: leader()}
      # RPCs timed out
      return accept_result_t{type_: retry, leader_: N/A}

    - accept_handler(message: accept_rpc_request)
      if message.ballot_ >= ballot_:
        ballot_ = message.ballot_
        instance = instance_t{ballot_: message.ballot_,
                              command_: message.command_,
                              index_: message.index_,
                              state_: in-progress
                              client-id_: message.client-id_}
        log_.append(instance)
        return accept_rpc_response{type_: ok, ballot_: N/A}
      # stale message
      return accept_rpc_response{type: reject, ballot: ballot_}

    - prepare() -> prepare_result_t:
      num_responses = 0
      ok_logs = vector<instance_t[]>
      cv, mu
      request = prepare_rpc_request{ballot_: next_ballot#()}
      for each peer p {
        run closure in a separate thread {
          response = p.prepareRPC(request)
          lock(mu)
          ++num_responses
          if response.type_ == ok:
            ok_logs.push(response.log_)
          else if response.type_ == reject:
            ballot_ = response.ballot_
          # else it is a timeout error; we do nothing
          unlock(mu)
          cv.notify_one()
        }
      }
      lock(mu)
      while i_am_leader() &&
            num_ok_responses <= peers_.size()/2 &&
            num_responses != peers_.size()
        cv.wait(mu)
      # one of the above three conditions is false; handle each, starting with the
      # most likely one
      if num_ok_responses > peers_.size()/2: # we have quorum
        return prepare_result_t{type_: ok, log_: ok_logs}
      if someone_else_is_leader():
        return prepare_result_t{type_: reject}
      # multiple timeout responses
      return prepare_result_t{type_: timeout}

    - prepare_handler(message: prepare_rpc_request):
      # common case for phase1
      if message.ballot >= ballot_:
        ballot_ = message.ballot_
        return prepare_rpc_response_t{type_: ok,
                                      ballot_: N/A,
                                      log_: log_.new_instances()}
      # stale messages
      return prepare_rpc_response_t{type_: reject, ballot_: ballot_, log_: N/A}

    - prepare_thread():
      for (;;) {
        sleep until follower
        for (;;) {
          sleep(heartbeat_interval_ + random(10, heartbeat_interval_))
          if time::now() - last_heartbeat_ < heartbeat_interval_:
            continue
          prepare_result_t result = prepare()
          if result.type_ != ok:
            continue
          # we are a leader
          wake up heartbeat_thread
          ready_ = false
          log_.merge(result.logs_)
          if (replay())
            ready_ = true
          break
        }
      }

    - replay() -> bool
      for i in log_.new_instances()
        accept_result_t r = accept(i.command_, i.index, i.client-id_)
        if r.type_ == leader
          return false
        if r.type_ == retry
          continue
      return true

    - heartbeat_thread():
      for (;;) {
        sleep until leader
        num_responses = 0
        ok_responses = vector
        cv, mu
        global_last_executed = log_.global_last_executed()
        for (;;) {
          request = heartbeat_rpc_request{ballot_: ballot_,
                                          last_executed_: log_.last_executed()
                                          global_last_executed_: global_last_executed}
          for each peer p {
            run closure in a separate thread {
              response = p.heartbeatRPC(request)
              lock(mu)
              ++num_responses
              if response.ok:
                ok_responses.push(response)
              unlock(mu)
              cv.notify_one()
            }
          }
          lock(mu)
          while i_am_leader() && num_responses != peers_.size():
            cv.wait(mu)
          if ok_responses.size() == peers_.size():
            global_last_executed = min(ok_responses)
          if someone_else_is_leader():
            break
          sleep(heartbeat_interval_)
        }
      }

    - heartbeat_handler(message: heartbeat_rpc_request):
      if message.ballot >= ballot_:
        last_heartbeat_ = time::now()
        ballot_ = message.ballot_
        log_.commit_until(message_.last_executed_, ballot_)
        log_.trim_until(message_.global_last_executed_)
      # stale message
      return heartbeat_rpc_response{last_executed_: log_.last_executed()}

== TODO ========================================================================

- we can handle gaps if there is a leader election, but if there is no leader
  election, a follower that temporarily experienced a network partition will
  hinder global progress. we need to come up with an alternative recovery
  mechanism to handle this problem.

- How to handle gaps?

  Currently, if a peer temporarily disconnects and then reconnects, then it will
  have a gap in its log. it will not be able to execute entries past the gap, it
  will not be able to prune its log, which will prevent everyone else from
  pruning their logs. when we have a gap like this, we should recover it by
  asking other peers. or we should resort to using log pruning that persists the
  state machine to disk and prunes the log without hearing from the peers. we do
  not implement this at the moment: if a peer temporarily disconnects and
  accrues a gap, then log pruning will be stuck on all processes.

- how to let peers know the committed? we can do it with the heartbeat, but
  should we, given that we already let everyone know executed entries?

  - the difference between global_last_executed_ and the committed entries is
    that we can only communicate global_last_executed_ if we have received the
    tails of all peers, whereas we can communicate the committed entries once we
    have the responses from the majority.

- handle duplicate responses due to retries

  - we will handle this by having gRPC retry RPC calls.

- imagine a scenario that there is a gap in the log, like [a, b, _, d] and once
  the thread1 commits d, it starts to wait until command at index 2 is executed
  and thread1 is woken up. at that moment, this machine stops being a leader,
  and someone else starts to run. they receive the log state, and eventually,
  they determine what goes into 2, and eventually, they notify this peer about
  the state of the log. then, we should wake up thread1)

- evaluate the choice of a resizeable circular buffer (see how boost implements
  it) for log on the performance.

- evaluate the choice of not sending messages to self on performance.

- evaluate the choice of lazy (via piggybacking onto heartbeats) vs eager (via
  piggybacking onto accepts) sending commit messages to followers.

== SCRATCH SPACE ===============================================================

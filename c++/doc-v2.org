* TYPES

- struct Result
  - synopsis: result of a key-value command
  - fields:
    - ok_: bool (true if result is a success, false otherwise)
    - value_: string (if ok_ == false, contains error message, otherwise
      contains return value for a get command or empty for put and del)

- struct accept_result_t
  - synopsis: return type of replicate() and accept()
  - fields:
    - type_: enum {ok, reject, retry}
    - leader_: int (if type_ == reject then contains id of the current leader_)

- struct prepare_result_t
  - synopsis: return type of prepare()
  - fields:
    - type_: enum {ok, reject, timeout}
    - logs_: vector<instance_t[]>

* OBJECTS

- Replicant

  - members
    - kv_: KVStore
    - c_: Consensus
    - log_: Log
    - tp_: Threadpool
    - l_: socket
    - map_: map from client_id(int) to socket object
    - client_id_: id of the next client that will connect
    - id_: id of the peers
    - num_peers_: number of peers

  - methods

    - constructor(cfg: config)
      kv_ = new(KVStore)
      c_ = new(Consensus(cfg))
      log_ = new(Log())
      id_ = config["id"]
      client_id_ = id_
      num_peers_ = confg["num_peers"]

    - next_client_id(void) -> int
      id = client_id_
      client_id_ += num_peers_
      return id

    - run(void)
      thread(executor).start()
      for (;;) {
        socket = accept(l)
        client-id = next_client_id()
        map_[client-id] = socket
        thread(handle_client, client-id).start()
      }

    - handle_client(client-id: int)
      it = map_[client-id]
      for (;;) {
        cmd = read_command(it->socket)
        if (cmd)
          tp_.post(replicate, cmd, client-id)
        else
          break
      }
      close(it->socket)
      del it

    - replicate(cmd: command, client-id: int)
      r = c.replicate(cmd, client-id)
      if r.type_ != ok
        it = map_[client-id]
        send(it->socket, r)
        close(it->socket)
        del it

    - executor(void)
      for (;;) {
        client-id, result = log_.execute(kv)
        respond(client-id, result)
      }

    - respond(client-id, result)
      # responds to the client with the result of the command execution. this
      # function will respond to the client only if the client originally sent
      # the request to this peer when it was a leader. this constraint is
      # implicitly enforced by having each peer assign a unique id to each
      # client.
      it = map_[client-id]
      if it != null
        send(it->socket, result)

== TODO ========================================================================

- we can handle gaps if there is a leader election, but if there is no leader
  election, a follower that temporarily experienced a network partition will
  hinder global progress. we need to come up with an alternative recovery
  mechanism to handle this problem.

- How to handle gaps?

  Currently, if a peer temporarily disconnects and then reconnects, then it will
  have a gap in its log. it will not be able to execute entries past the gap, it
  will not be able to prune its log, which will prevent everyone else from
  pruning their logs. when we have a gap like this, we should recover it by
  asking other peers. or we should resort to using log pruning that persists the
  state machine to disk and prunes the log without hearing from the peers. we do
  not implement this at the moment: if a peer temporarily disconnects and
  accrues a gap, then log pruning will be stuck on all processes.

- how to let peers know the committed? we can do it with the heartbeat, but
  should we, given that we already let everyone know executed entries?

  - the difference between global_last_executed_ and the committed entries is
    that we can only communicate global_last_executed_ if we have received the
    tails of all peers, whereas we can communicate the committed entries once we
    have the responses from the majority.

- handle duplicate responses due to retries

  - we will handle this by having gRPC retry RPC calls.

- imagine a scenario that there is a gap in the log, like [a, b, _, d] and once
  the thread1 commits d, it starts to wait until command at index 2 is executed
  and thread1 is woken up. at that moment, this machine stops being a leader,
  and someone else starts to run. they receive the log state, and eventually,
  they determine what goes into 2, and eventually, they notify this peer about
  the state of the log. then, we should wake up thread1)

- evaluate the choice of a resizeable circular buffer (see how boost implements
  it) for log on the performance.

- evaluate the choice of not sending messages to self on performance.

- evaluate the choice of lazy (via piggybacking onto heartbeats) vs eager (via
  piggybacking onto accepts) sending commit messages to followers.

== SCRATCH SPACE ===============================================================
